{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"RELAI: Simulate \u2192 Evaluate \u2192 Optimize AI Agents <p>RELAI is an SDK for building reliable AI agents. It streamlines the hardest parts of agent development\u2014simulation, evaluation, and optimization\u2014so you can iterate quickly with confidence.</p> <p>What you get</p> <ul> <li> <p>Agent Simulation \u2014 Create full/partial environments, define LLM personas, mock MCP servers &amp; tools, and generate synthetic data. Optionally condition simulation on real samples to better match production.</p> </li> <li> <p>Agent Evaluation \u2014 Mix code-based and LLM-based custom evaluators or use RELAI platform evaluators. Turn human reviews into benchmarks you can re-run.</p> </li> <li> <p>Agent Optimization (Maestro) \u2014 Holistic optimizer that uses evaluator signals &amp; feedback to improve prompts/configs and suggest graph-level changes. Also selects best model/tool/graph based on observed performance.</p> </li> </ul>"},{"location":"index.html#quick-links","title":"Quick Links","text":"<p>Get up and running with RELAI in minutes:</p> <ul> <li>Getting Started - Installation, setup and code walkthrough</li> <li>Tutorials - Tutorials that show how to achieve a specific task or using a particular feature</li> <li>Examples - Self-contained examples illustrating using the SDK in various common scenarios</li> <li>API Reference - Detailed reference to the SDK API</li> </ul>"},{"location":"index.html#getting-started","title":"Getting Started","text":""},{"location":"index.html#installation","title":"Installation","text":"<p>You can install the RELAI SDK with using your favorite Python package manager (requires Python 3.9+):</p> <pre><code>pip install relai\n# or\nuv add relai\n</code></pre>"},{"location":"index.html#setting-the-relai-api-key","title":"Setting the RELAI API key","text":"<p>A RELAI API key is necessary to use features from the RELAI platform. You can get a RELAI API key from your RELAI enterprise dashboard. After you copy the key, assign to <code>RELAI_API_KEY</code> environment variable:</p> <pre><code>export RELAI_API_KEY=\"relai-...\"\n</code></pre>"},{"location":"index.html#building-reliable-ai-agents-with-relai-sdk","title":"Building Reliable AI Agents with RELAI SDK","text":""},{"location":"index.html#step-1-decorate-inputstools-that-will-be-simulated","title":"Step 1 \u2014 Decorate inputs/tools that will be simulated","text":"<pre><code>from relai.mocker import Persona, MockTool\nfrom relai.simulator import simulated\nfrom agents import function_tool\n\nAGENT_NAME = \"Stock Chatbot\"\nMODEL = \"gpt-5-mini\"\n\n\n# Decorate functions to be mocked in the simulation\n@simulated\nasync def get_user_query() -&gt; str:\n    \"\"\"Get user's query about stock prices.\"\"\"\n    return \"What is the current price of AAPL stock?\"\n\n\n@function_tool\n@simulated\nasync def retriever(query: str) -&gt; list[str]:\n    \"\"\"\n    A retriever tool that returns relevant financial data for a given query about stock prices.\n    \"\"\"\n    return []\n</code></pre>"},{"location":"index.html#step-2-register-params-to-be-optimized-and-define-your-agent","title":"Step 2 \u2014 Register params to be optimized and define your agent","text":"<pre><code>from agents import Agent, Runner\nfrom relai.maestro import params, register_param\n\nregister_param(\n    \"prompt\",\n    type=\"prompt\",\n    init_value=\"You are a helpful assistant for stock price questions.\",\n    desc=\"system prompt for the agent\",\n)\n\nasync def stock_price_chatbot(question: str) -&gt; dict[str, str]:\n    agent = Agent(\n        name=AGENT_NAME,\n        instructions=params.prompt,  # access registered parameter\n        model=MODEL,\n        tools=[retriever],\n    )\n    result = await Runner.run(agent, question)\n    return {\"answer\": result.final_output}\n</code></pre>"},{"location":"index.html#step-3-wrap-agent-for-simulation-traces","title":"Step 3 \u2014 Wrap agent for simulation traces","text":"<pre><code>from relai import AgentOutputs, SimulationTape\n\nasync def agent_fn(tape: SimulationTape) -&gt; AgentOutputs:\n    question = await get_user_query()\n    tape.agent_inputs[\"question\"] = question  # trace inputs for later auditing\n    return await stock_price_chatbot(question)\n</code></pre>"},{"location":"index.html#step-4-define-evaluators","title":"Step 4 - Define evaluators","text":"<pre><code>import re\nfrom relai import AgentLog, EvaluatorLog\nfrom relai.critico.evaluate import Evaluator\n\nclass PriceFormatEvaluator(Evaluator):\n    \"\"\"Checks for correct price formats ($\u2026 with exactly two decimals).\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(name=\"PriceFormatEvaluator\", required_fields=[\"answer\"])\n\n    async def compute_evaluator_result(self, agent_log: AgentLog) -&gt; EvaluatorLog:\n        # flag $-prices that are NOT like $1,234.56 or $1234.56\n        bad_pattern = r\"\\$(?!\\d{1,3}(?:,\\d{3})+|\\d+\\.\\d{2}\\b)\\S+\"\n        bad_prices = re.findall(bad_pattern, agent_log.agent_outputs[\"answer\"])\n        score = 0.0 if bad_prices else 1.0\n        feedback = (\n            (\"Incorrect price formats found: \" + \", \".join(bad_prices))\n            if bad_prices else\n            \"Price formats look good.\"\n        )\n        return EvaluatorLog(\n            evaluator_id=self.uid,\n            name=self.name,\n            outputs={\"score\": score, \"feedback\": feedback},\n        )\n</code></pre>"},{"location":"index.html#step-5-orchestrate-simulate-evaluate-optimize","title":"Step 5 - Orchestrate: simulate \u2192 evaluate \u2192 optimize","text":"<pre><code>import asyncio\n\nfrom relai import AsyncRELAI, AsyncSimulator, random_env_generator\nfrom relai.critico import Critico\nfrom relai.maestro import Maestro, params\nfrom relai.mocker import Persona, MockTool  # (already imported in Step 1 if single file)\n\nasync def main() -&gt; None:\n    # 5.1 \u2014 Set up your simulation environment\n    env_generator = random_env_generator(\n        config_set={\n            \"__main__.get_user_query\": [Persona(user_persona=\"A polite and curious user.\")],\n            \"__main__.retriever\": [MockTool(model=MODEL)],\n        }\n    )\n\n    async with AsyncRELAI() as client:\n        # 5.2 \u2014 SIMULATE\n        simulator = AsyncSimulator(agent_fn=agent_fn, env_generator=env_generator, client=client)\n        agent_logs = await simulator.run(num_runs=1)\n\n        # 5.3 \u2014 EVALUATE\n        critico = Critico(client=client)\n        critico.add_evaluators({PriceFormatEvaluator(): 1.0})\n        critico_logs = await critico.evaluate(agent_logs)\n\n        # Publish evaluation report to the RELAI platform\n        await critico.report(critico_logs)\n\n        # 5.4 \u2014 OPTIMIZE with Maestro\n        maestro = Maestro(client=client, agent_fn=agent_fn, log_to_platform=True, name=AGENT_NAME)\n        maestro.add_setup(simulator=simulator, critico=critico)\n\n        # 5.4.1 \u2014 Optimize agent configurations\n        # params.load(\"saved_config.json\")  # load previous params if available\n        await maestro.optimize_config(\n            total_rollouts=50,\n            batch_size=2,\n            explore_radius=5,\n            explore_factor=0.5,\n            verbose=True,\n        )\n        params.save(\"saved_config.json\")  # save optimized params for future usage\n\n        # 5.4.2 \u2014 Optimize agent structure\n        await maestro.optimize_structure(\n            total_rollouts=10,\n            code_paths=[\"agentic-rag.py\"],\n            verbose=True,\n        )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"api/index.html","title":"Index","text":"API Reference <ul> <li>Benchmark: Data abstractions for packaging annotated samples into reusable benchmarks and CSV-backed suites.</li> <li>Critico: Orchestrator that aggregates evaluator results and reports back to the RELAI platform.</li> <li>Evaluator: Base classes and built-in evaluators for rubric, format, style, and annotation scoring.</li> <li>Maestro: Optimization engine that tunes agent configs and structure based on evaluation feedback.</li> <li>Mockers: Persona and mock tool definitions for simulating MCP interactions during tests.</li> <li>Simulator: Decorators and simulator runtimes to replay agent flows in controlled environments.</li> <li>Types: Core data models (<code>RELAISample</code>, <code>SimulationTape</code>, logs) shared across simulation and evaluation.</li> </ul>"},{"location":"api/benchmark.html","title":"Benchmark","text":""},{"location":"api/benchmark.html#relai.benchmark.Benchmark","title":"<code>relai.benchmark.Benchmark(benchmark_id, samples=None)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for defining and managing benchmarks.</p> <p>This class provides a foundational structure for benchmarks, enabling the download, and iteration of samples. It ensures that all concrete benchmark implementations have a unique identifier and a collection of samples to be used as inputs for AI agents and evaluators.</p> <p>Attributes:</p> Name Type Description <code>benchmark_id</code> <code>str</code> <p>A unique identifier for this specific benchmark.</p> <code>samples</code> <code>list[RELAISample]</code> <p>A list of <code>RELAISample</code> objects contained within this benchmark. Defaults to an empty list if not provided.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_id</code> <code>str</code> <p>The unique identifier for the benchmark.</p> required <code>samples</code> <code>list[RELAISample]</code> <p>A list of <code>RELAISample</code> objects to include in the benchmark. Defaults to an empty list.</p> <code>None</code>"},{"location":"api/benchmark.html#relai.benchmark.Benchmark.__iter__","title":"<code>__iter__()</code>","text":"<p>Enables iteration over the samples within the benchmark as follows:</p> <pre><code>for sample in benchmark:\n    # Process each sample\n    pass\n</code></pre> <p>Yields:</p> Name Type Description <code>RELAISample</code> <code>RELAISample</code> <p>Each <code>RELAISample</code> object contained in the benchmark.</p>"},{"location":"api/benchmark.html#relai.benchmark.Benchmark.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of samples currently in the benchmark.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The total count of <code>Sample</code> objects.</p>"},{"location":"api/benchmark.html#relai.benchmark.Benchmark.sample","title":"<code>sample(n=1)</code>","text":"<p>Returns <code>n</code> random samples from the benchmark, with replacement.</p> <p>If <code>n</code> is greater than the total number of samples, samples may be repeated in the returned list.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of random samples to retrieve. Must be a positive integer. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[RELAISample]</code> <p>list[RELAISample]: A list containing <code>n</code> randomly selected <code>RELAISample</code> objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>n</code> is less than or equal to 0.</p>"},{"location":"api/benchmark.html#relai.benchmark.RELAIBenchmark","title":"<code>relai.benchmark.RELAIBenchmark(benchmark_id, field_name_mapping=None, field_value_transform=None, agent_input_fields=None, extra_fields=None)</code>","text":"<p>               Bases: <code>Benchmark</code></p> <p>A concrete implementation of <code>Benchmark</code> that downloads samples from the RELAI platform.</p> <p>Attributes:</p> Name Type Description <code>benchmark_id</code> <code>str</code> <p>The unique identifier (ID) of the RELAI benchmark to be loaded from the platform. You can find the benchmark ID in the metadata of the benchmark.</p> <code>samples</code> <code>list[RELAISample]</code> <p>A list of <code>RELAISample</code> objects contained within this benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_id</code> <code>str</code> <p>The unique identifier for the RELAI benchmark. This ID is used to fetch the benchmark data from the RELAI platform.</p> required <code>field_name_mapping</code> <code>dict[str, str]</code> <p>A mapping from field names returned by the RELAI API to standardized field names expected by the evaluators. If a field name is not present in this mapping, it is used as-is. Defaults to an empty dictionary.</p> <code>None</code> <code>field_value_transform</code> <code>dict[str, Callable]</code> <p>A mapping from field names to transformation functions that convert field values from the RELAI API into the desired format. If a field name is not present in this mapping, the identity function is used (i.e., no transformation). Defaults to an empty dictionary.</p> <code>None</code> <code>agent_input_fields</code> <code>list[str]</code> <p>A list of field names to extract from each sample for the <code>agent_inputs</code> dictionary. These fields are provided to the AI agent. Defaults to an empty list.</p> <code>None</code> <code>extra_fields</code> <code>list[str]</code> <p>A list of field names to extract from each sample for the <code>extras</code> dictionary. These fields are also provided to the evaluators. Defaults to an empty list.</p> <code>None</code>"},{"location":"api/benchmark.html#relai.benchmark.RELAIBenchmark.fetch_samples","title":"<code>fetch_samples()</code>","text":"<p>Downloads samples from the RELAI platform and populates the <code>samples</code> attribute.</p> <p>This method fetches the benchmark data using the RELAI client and processes each sample to create <code>Sample</code> objects. The <code>samples</code> attribute is then updated with the newly fetched samples.</p>"},{"location":"api/benchmark.html#relai.benchmark.RELAIQuestionAnsweringBenchmark","title":"<code>relai.benchmark.RELAIQuestionAnsweringBenchmark(benchmark_id)</code>","text":"<p>               Bases: <code>RELAIBenchmark</code></p> <p>A concrete implementation of <code>RELAIBenchmark</code> for question-answering tasks. All samples in this benchmark have the following fields:</p> <ul> <li><code>agent_inputs</code>:<ul> <li><code>question</code>: The question to be answered by the AI agent.</li> </ul> </li> <li><code>extras</code>:<ul> <li><code>rubrics</code>:  A dictionary of rubrics for evaluating the answer.</li> <li><code>std_answer</code>: The standard answer to the question. }</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>benchmark_id</code> <code>str</code> <p>The unique identifier for the RELAI question-answering benchmark. This ID is used to fetch the benchmark data from the RELAI platform.</p> required"},{"location":"api/benchmark.html#relai.benchmark.RELAISummarizationBenchmark","title":"<code>relai.benchmark.RELAISummarizationBenchmark(benchmark_id)</code>","text":"<p>               Bases: <code>RELAIBenchmark</code></p> <p>A concrete implementation of <code>RELAIBenchmark</code> for summarization tasks. All samples in this benchmark have the following fields:</p> <ul> <li><code>agent_inputs</code>:<ul> <li><code>source</code>: The text to be summarized.</li> </ul> </li> <li><code>extras</code>:<ul> <li><code>key_facts</code>: A list of key facts extracted from the source.</li> <li><code>style_rubrics</code>: A dictionary of rubrics for evaluating the style of the summary.</li> <li><code>format_rubrics</code>: A dictionary of rubrics for evaluating the format of the summary.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>benchmark_id</code> <code>str</code> <p>The unique identifier for the RELAI summarization benchmark. This ID is used to fetch the benchmark data from the RELAI platform.</p> required"},{"location":"api/benchmark.html#relai.benchmark.RELAIAnnotationBenchmark","title":"<code>relai.benchmark.RELAIAnnotationBenchmark(benchmark_id)</code>","text":"<p>               Bases: <code>RELAIBenchmark</code></p> <p>A concrete implementation of <code>RELAIBenchmark</code> for benchmarks created from user annotations. All samples in this benchmark have the following fields:</p> <ul> <li><code>agent_inputs</code>:<ul> <li>The input(s) provided to the agent being evaluated.</li> </ul> </li> <li><code>extras</code>:<ul> <li><code>previous_outputs</code>: The previous outputs produced by the agent.</li> <li><code>desired_outputs</code>: The desired outputs as specified by the user.</li> <li><code>feedback</code>: The user feedback provided for the previous outputs.</li> <li><code>liked</code>: A boolean indicating whether the user liked the previous outputs.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>benchmark_id</code> <code>str</code> <p>The unique identifier for the RELAI summarization benchmark. This ID is used to fetch the benchmark data from the RELAI platform.</p> required"},{"location":"api/benchmark.html#relai.benchmark.CSVBenchmark","title":"<code>relai.benchmark.CSVBenchmark(csv_file, agent_input_columns=None, extra_columns=None, benchmark_id=None)</code>","text":"<p>               Bases: <code>Benchmark</code></p> <p>A concrete implementation of <code>Benchmark</code> that loads samples from a CSV file.</p> <p>Attributes:</p> Name Type Description <code>benchmark_id</code> <code>str</code> <p>The unique identifier (ID) of the benchmark to be loaded from the CSV file. Defaults to the CSV file name.</p> <code>samples</code> <code>list[Sample]</code> <p>A list of <code>Sample</code> objects contained within this benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>The path to the CSV file containing benchmark samples.</p> required <code>agent_input_columns</code> <code>list[str]</code> <p>A list of column names in the CSV file that should be used as inputs for the AI agent. Defaults to an empty list.</p> <code>None</code> <code>extra_columns</code> <code>list[str]</code> <p>A list of column names in the CSV file that could be used as inputs for evaluators. Defaults to an empty list.</p> <code>None</code> <code>benchmark_id</code> <code>str</code> <p>A unique identifier for the benchmark. If not provided, it defaults to the name of the CSV file.</p> <code>None</code>"},{"location":"api/critico.html","title":"Critico","text":""},{"location":"api/critico.html#relai.critico.Critico","title":"<code>relai.critico.Critico(client)</code>","text":"<p>Critico orchestrates evaluation of an AI agent using a configurable set of evaluators.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required"},{"location":"api/critico.html#relai.critico.Critico.add_evaluators","title":"<code>add_evaluators(evaluators, evaluator_group='default')</code>","text":"<p>Adds a new evaluator to Critico to a specific evaluator group.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator_group</code> <code>str</code> <p>The name of the evaluator group.</p> <code>'default'</code> <code>evaluators</code> <code>dict[Evaluator, float]</code> <p>A dictionary where keys are <code>Evaluator</code> objects and values are their corresponding weights for this evaluator group. If <code>None</code>, no evaluators are associated with this evaluator group initially. Defaults to <code>None</code>.</p> required"},{"location":"api/critico.html#relai.critico.Critico.evaluate","title":"<code>evaluate(agent_logs)</code>  <code>async</code>","text":"<p>Evaluates a list of AI agent logs against their corresponding evaluator groups using the configured evaluators.</p> <p>For each <code>AgentLog</code>, Critico identifies the associated evaluator group and runs all evaluators configured for that group. It then aggregates the individual evaluator results (scores and feedback) into a single <code>CriticoFeedback</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>agent_logs</code> <code>list[AgentLog]</code> <p>A list of <code>AgentLog</code> objects to be evaluated. Each log must contain a <code>simulation_tape.evaluator_group</code> that corresponds to an evaluator group added to Critico.</p> required <p>Returns:</p> Type Description <code>list[CriticoLog]</code> <p>list[CriticoFeedback]: A list of <code>CriticoFeedback</code> objects, where each object summarizes the evaluation outcome for one <code>AgentLog</code>.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a <code>evaluator_group</code> within an <code>AgentLog</code> is not found       among Critico's managed evaluator groups.</p>"},{"location":"api/critico.html#relai.critico.Critico.report","title":"<code>report(critico_logs)</code>  <code>async</code>","text":"<p>Submits the critico logs (as multiple <code>CriticoLog</code> objects) to the RELAI platform.</p> <p>Parameters:</p> Name Type Description Default <code>critico_logs</code> <code>list[CriticoLog]</code> <p>A list of <code>CriticoLog</code> objects containing the evaluation results for each <code>AgentLog</code>.</p> required <p>Raises:</p> Type Description <code>RELAIError</code> <p>If any <code>CriticoLog</code> does not have a valid <code>trace_id</code>.</p>"},{"location":"api/critico.html#relai.critico.Critico.reweight_evaluator","title":"<code>reweight_evaluator(evaluator_group, evaluator, weight)</code>","text":"<p>Adjusts the weight of a specific evaluator in a given evaluator group.</p> <p>Evaluator weights influence their contribution to the aggregate score during the <code>evaluate()</code> operation.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator_group</code> <code>str</code> <p>The name of the evaluator group.</p> required <code>evaluator</code> <code>Evaluator</code> <p>The <code>Evaluator</code> object whose weight is to be adjusted.</p> required <code>weight</code> <code>float</code> <p>The new weight (a positive float) for the evaluator. Higher weights give more prominence to its results in aggregation.</p> required <p>Raises:</p> Type Description <code>RELAIError</code> <p>If the <code>evaluator_group</code> is not found in Critico, or if the <code>evaluator</code> is not associated with the specified <code>evaluator_group</code>.</p> <code>ValueError</code> <p>If the specified <code>weight</code> is not a positive float.</p>"},{"location":"api/evaluator.html","title":"Evaluator","text":""},{"location":"api/evaluator.html#relai.critico.evaluate.Evaluator","title":"<code>relai.critico.evaluate.Evaluator(name, required_fields=None, transform=None, **hyperparameters)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for defining and implementing evaluators for a benchmark.</p> <p>Evaluators are responsible for assessing an AI agent's response to a specific benchmark sample. They can define required input fields from the <code>AgentLog</code> necessary for their evaluation logic and may incorporate customizable hyperparameters to tune their behavior.</p> <p>Subclasses must implement the <code>compute_evaluator_result</code> method to define their specific evaluation logic, which produces an <code>EvaluatorResponse</code> object.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the evaluator, used for identification.</p> <code>required_fields</code> <code>list[str]</code> <p>A list of field names (keys) that must be present in either <code>agent_inputs</code> (of the sample), <code>eval_inputs</code> (of the sample), or <code>agent_outputs</code> (of the agent log).</p> <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>hyperparameters</code> <code>dict[str, Any]</code> <p>A dictionary of arbitrary keyword arguments passed during initialization, allowing for custom configuration of the evaluator's behavior.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The display name of the evaluator, used to identify the evaluator in evaluation results.</p> required <code>required_fields</code> <code>list[str]</code> <p>A list of field names (keys) that must be present in either <code>agent_inputs</code> (of the sample), <code>eval_inputs</code> (of the sample), or <code>agent_outputs</code> (of the agent log).</p> <code>None</code> <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code> <code>hyperparameters</code> <code>dict[str, Any]</code> <p>A dictionary of arbitrary keyword arguments passed during initialization, allowing for custom configuration of the evaluator's behavior.</p> <code>{}</code>"},{"location":"api/evaluator.html#relai.critico.evaluate.Evaluator.uid","title":"<code>uid</code>  <code>cached</code> <code>property</code>","text":"<p>Generates a unique identifier for this specific evaluator instance. The UID is constructed from the evaluator's class name combined with a JSON-serialized representation of its hyperparameters.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A unique identifier for the evaluator.</p>"},{"location":"api/evaluator.html#relai.critico.evaluate.Evaluator.__call__","title":"<code>__call__(agent_log)</code>  <code>async</code>","text":"<p>Executes the evaluation process for a given AI agent log.</p> <p>Parameters:</p> Name Type Description Default <code>agent_log</code> <code>AgentLog</code> <p>The response from the AI agent to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>EvaluatorResponse</code> <code>EvaluatorLog</code> <p>The structured result of the evaluation, including any computed <code>score</code> and <code>feedback</code>, as defined by the concrete evaluator.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>agent_log</code> is not an instance of <code>AgentLog</code> or <code>agent_outputs</code> (after transform) in agent_log is not a dict.</p> <code>ValueError</code> <p>If any <code>required_fields</code> are missing from the <code>agent_log</code>.</p>"},{"location":"api/evaluator.html#relai.critico.evaluate.Evaluator.__hash__","title":"<code>__hash__()</code>","text":"<p>Computes the hash value for the evaluator based on its unique identifier (<code>uid</code>).</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The hash value of the evaluator's unique identifier.</p>"},{"location":"api/evaluator.html#relai.critico.evaluate.Evaluator.compute_evaluator_result","title":"<code>compute_evaluator_result(agent_log)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Abstract method: Computes the evaluation result for an agent log.</p> <p>Concrete subclasses must implement this method to define their unique evaluation logic. This method should process the <code>AgentLog</code> by accessing its <code>sample</code> and <code>agent_outputs</code> to derive the evaluation outcome, which is then encapsulated in an <code>EvaluatorResponse</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>agent_log</code> <code>AgentLog</code> <p>The comprehensive response from the AI agent, including the original sample and agent's outputs.</p> required <p>Returns:</p> Name Type Description <code>EvaluatorResponse</code> <code>EvaluatorLog</code> <p>An instance of <code>EvaluatorResponse</code> containing the evaluation outcome, typically including a <code>score</code> and/or <code>feedback</code>, along with <code>evaluator_name</code>, <code>evaluator_configuration</code>, and the original <code>agent_log</code>.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If a concrete subclass does not override this method.</p>"},{"location":"api/evaluator.html#relai.critico.evaluate.RELAIEvaluator","title":"<code>relai.critico.evaluate.RELAIEvaluator(client, relai_evaluator_name, name, required_fields=None, transform=None, **hyperparameters)</code>","text":"<p>               Bases: <code>Evaluator</code></p> <p>Base class for all RELAI evaluators that use the RELAI API to evaluate responses on a benchmark.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the specific evaluator to be invoked on the RELAI platform.</p> <p>Initializes a new RELAIEvaluator instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>relai_evaluator_name</code> <code>str</code> <p>The name of the RELAI evaluator to be used for evaluation.</p> required <code>name</code> <code>str</code> <p>The display name of the evaluator, used to identify the evaluator in evaluation results.</p> required <code>required_fields</code> <code>list[str]</code> <p>A list of field names that must be present in the <code>AgentLog</code> (across agent inputs, eval inputs, or agent outputs). Defaults to an empty list.</p> <code>None</code> <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code> <code>**hyperparameters</code> <code>Any</code> <p>Arbitrary keyword arguments passed to the base <code>Evaluator</code> class and also forwarded to the RELAI evaluator.</p> <code>{}</code>"},{"location":"api/evaluator.html#relai.critico.evaluate.RELAIEvaluator.compute_evaluator_result","title":"<code>compute_evaluator_result(agent_log)</code>  <code>async</code>","text":"<p>Computes the structured evaluation result by invoking a RELAI evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>agent_log</code> <code>AgentLog</code> <p>The response from the AI agent.</p> required <p>Returns:</p> Name Type Description <code>EvaluatorResponse</code> <code>EvaluatorLog</code> <p>A structured evaluation result containing the evaluator's unique ID, the original agent log, and an optional <code>score</code> and <code>feedback</code> computed by the RELAI evaluator.</p>"},{"location":"api/evaluator.html#relai.critico.evaluate.RELAILengthEvaluator","title":"<code>relai.critico.evaluate.RELAILengthEvaluator(client, measure='words', use_ratio=False, acceptable_range=None, target_ratio=None, slope=1.0, temperature=1.0, transform=None)</code>","text":"<p>               Bases: <code>RELAIEvaluator</code></p> <p>Evaluator to assess the length of generated text (e.g., summaries) using a RELAI evaluator. Supports evaluating length in as measured by number of characters, words, or sentences, or based on the compression ratio.</p> <p>Required fields:</p> <pre><code>- `source`: The original text or document from which the summary is derived.\n- `summary`: The generated summary to be evaluated.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>measure</code> <code>str</code> <p>The unit for length calculation; one of: - 'characters': count every character, - 'words': split on whitespace, - 'sentences': split on sentence-ending punctuation (., !, ?). Defaults to 'words'.</p> <code>'words'</code> <code>use_ratio</code> <code>bool</code> <p>If True, ignore <code>acceptable_range</code> and instead evaluate the length based on the compression ratio: <code>1 - (summary_length / source_length)</code> relative to <code>target_ratio</code>. Defaults to False.</p> <code>False</code> <code>acceptable_range</code> <code>tuple[int, int]</code> <p>A two-element tuple <code>(min_len, max_len)</code> specifying the inclusive bounds for the length of <code>summary</code> under the chosen <code>measure</code>. Required if <code>use_ratio</code> is False. Ignored if <code>use_ratio</code> is True. Defaults to None.</p> <code>None</code> <code>target_ratio</code> <code>float</code> <p>The desired summary-to-source length ratio (between 0 and 1). Required if <code>use_ratio</code> is True. Defaults to None.</p> <code>None</code> <code>slope</code> <code>float</code> <p>A factor in [0, 1] controlling the penalty slope for summaries shorter than the lower bound. A slope of 1.0 yields a linear ramp from 0 at zero length to 1.0 at <code>min_len</code>. Defaults to 1.0.</p> <code>1.0</code> <code>temperature</code> <code>float</code> <p>A positive scaling factor that smooths the exponential penalty for summaries exceeding the upper bound. Higher values make the penalty curve flatter. Defaults to 1.0.</p> <code>1.0</code> <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the parameters are invalid.</p>"},{"location":"api/evaluator.html#relai.critico.evaluate.RELAIContentEvaluator","title":"<code>relai.critico.evaluate.RELAIContentEvaluator(client, transform=None)</code>","text":"<p>               Bases: <code>RELAIEvaluator</code></p> <p>Evaluator for assessing the factual content of a generated summary against provided key facts, using a RELAI evaluator.</p> <p>Required fields:</p> <pre><code>- `key_facts`: A dictionary of key facts with their associated weights, which the summary should cover.\n- `summary`: The generated summary to be evaluated.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code>"},{"location":"api/evaluator.html#relai.critico.evaluate.RELAIHallucinationEvaluator","title":"<code>relai.critico.evaluate.RELAIHallucinationEvaluator(client, transform=None)</code>","text":"<p>               Bases: <code>RELAIEvaluator</code></p> <p>Evaluator for detecting factual inconsistencies or \"hallucinations\" in generated text (e.g., summaries) relative to a source document, using a RELAI evaluator.</p> <p>Required fields:</p> <pre><code>- `source`: The original text or document from which the summary is derived.\n- `summary`: The generated summary to be evaluated.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code>"},{"location":"api/evaluator.html#relai.critico.evaluate.RELAIStyleEvaluator","title":"<code>relai.critico.evaluate.RELAIStyleEvaluator(client, transform=None)</code>","text":"<p>               Bases: <code>RELAIEvaluator</code></p> <p>Evaluator for assessing the stylistic adherence of a generated summary based on provided rubrics, using a RELAI evaluator.</p> <p>Required fields:</p> <pre><code>- `style_rubrics`: A dictionary of style rubrics with their associated weights,\n    which the summary should adhere to.\n- `summary`: The generated summary to be evaluated.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code>"},{"location":"api/evaluator.html#relai.critico.evaluate.RELAIFormatEvaluator","title":"<code>relai.critico.evaluate.RELAIFormatEvaluator(client, transform=None)</code>","text":"<p>               Bases: <code>RELAIEvaluator</code></p> <p>Evaluator for assessing the formatting adherence of a generated summary based on provided rubrics, using a RELAI evaluator.</p> <p>Required fields:</p> <pre><code>- `format_rubrics`: A dictionary of format rubrics with their associated weights,\n    which the summary should adhere to.\n- `summary`: The generated summary to be evaluated.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code>"},{"location":"api/evaluator.html#relai.critico.evaluate.RELAIRubricBasedEvaluator","title":"<code>relai.critico.evaluate.RELAIRubricBasedEvaluator(client, transform=None)</code>","text":"<p>               Bases: <code>RELAIEvaluator</code></p> <p>Evaluator for performing a detailed, rubric-driven assessment of an AI agent's response to a query using an LLM-based evaluator on the RELAI platform.</p> <p>Required fields:</p> <pre><code>- `question`: The question or prompt that the AI agent was asked to respond to.\n- `answer`: The AI agent's generated response to the question.\n- `rubrics`: A dictionary of evaluation criteria with their associated weights,\n    which the answer should satisfy.\n- `std_answer`: The standard or expected answer against which the AI agent's response is evaluated.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code>"},{"location":"api/evaluator.html#relai.critico.evaluate.RELAIAnnotationEvaluator","title":"<code>relai.critico.evaluate.RELAIAnnotationEvaluator(client, transform=None)</code>","text":"<p>               Bases: <code>RELAIEvaluator</code></p> <p>Evaluator for assessing agent logs based on past human preference annotations provided through the RELAI platform.</p> <p>Required fields:</p> <pre><code>- `all_inputs`: The full set of inputs originally supplied to the agent.\n- `previous_outputs`: Prior agent output(s) shown to the human annotator.\n- `desired_outputs`: Human-preferred or target outputs provided by the annotator.\n- `feedback`: Free-text human feedback or rationale provided by the annotator.\n- `liked`: A flag indicating whether the annotator liked the agent's output.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code>"},{"location":"api/evaluator.html#relai.critico.evaluate.RELAICustomEvaluator","title":"<code>relai.critico.evaluate.RELAICustomEvaluator(evaluator_id, model_name='gpt-5-mini', transform=None)</code>","text":"<p>               Bases: <code>Evaluator</code></p> <p>Evaluator for assessing agent logs based on the custom evaluator prompt, input and output formats defined on the RELAI platform.</p> <p>Required fields:</p> <pre><code>- Any fields specified in the custom evaluator's input format (on the platform).\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>evaluator_id</code> <code>str</code> <p>The unique identifier of the custom evaluator defined on the RELAI platform.</p> required <code>model_name</code> <code>str</code> <p>The name of the model to use for the evaluator. Defaults to \"gpt-5-mini\".</p> <code>'gpt-5-mini'</code> <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code>"},{"location":"api/maestro.html","title":"Maestro","text":""},{"location":"api/maestro.html#relai.maestro.Maestro","title":"<code>relai.maestro.Maestro(client, agent_fn, goal=None, max_memory=20, name='No Name', log_to_platform=True)</code>","text":"<p>Maestro automatically optimizes an AI agent to maximize its Critico score, navigating the space of configurations to intelligently improve performance on the chosen criteria.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>agent_fn</code> <code>AsyncAgent</code> <p>The agent function to be optimized.</p> required <code>goal</code> <code>str</code> <p>Optional description of the goal of optimization. If None, increasing evaluation score will be considered as the only goal. Defaults to None.</p> <code>None</code> <code>max_memory</code> <code>int</code> <p>Control the maximum number of previous optimization history visible at each optimization step. Defaults to 20.</p> <code>20</code> <code>name</code> <code>str</code> <p>Name of the configuration optimization visualization on RELAI platform. Defaults to \"No Name\".</p> <code>'No Name'</code> <code>log_to_platform</code> <code>bool</code> <p>Whether to log optimization progress and results on RELAI platform. Defaults to True.</p> <code>True</code>"},{"location":"api/maestro.html#relai.maestro.Maestro.add_setup","title":"<code>add_setup(simulator, critico, weight=1)</code>","text":"<p>Add a new setup consisting of a simulator and a critico to Maestro.</p> <p>Parameters:</p> Name Type Description Default <code>simulator</code> <code>AsyncSimulator</code> <p>An AsyncSimulator to run the agent in the new setup.</p> required <code>critico</code> <code>Critico</code> <p>A Critico with evaluators for the new setup.</p> required <code>weight</code> <code>float</code> <p>A positive float representing the weight of this setup in comparson to others. Defaults to 1.</p> <code>1</code>"},{"location":"api/maestro.html#relai.maestro.Maestro.optimize_config","title":"<code>optimize_config(total_rollouts, batch_size=4, explore_radius=5, explore_factor=0.5, verbose=False)</code>  <code>async</code>","text":"<p>Optimize the configs (parameters) of the agent.</p> <p>Parameters:</p> Name Type Description Default <code>total_rollouts</code> <code>int</code> <p>Total number of rollouts to use for optimization.</p> required <code>batch_size</code> <code>int</code> <p>Base batch size to use for individual optimization steps. Defaults to 4.</p> <code>4</code> <code>explore_radius</code> <code>int</code> <p>A positive integer controlling the aggressiveness of exploration during optimization. A larger <code>explore_radius</code> encourages the optimizer to make more substantial changes between successive configurations. Defaults to 5.</p> <code>5</code> <code>explore_factor</code> <code>float</code> <p>A float between 0 to 1 controlling the exploration-exploitation trade-off. A higher <code>explore_factor</code> allocates more rollouts to discover new configs, while a lower value allocates more rollouts to ensure the discovered configs are thoroughly evaluated. Defaults to 0.5.</p> <code>0.5</code> <code>verbose</code> <code>bool</code> <p>If True, related information will be printed during the optimization step. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input parameters are not valid.</p>"},{"location":"api/maestro.html#relai.maestro.Maestro.optimize_structure","title":"<code>optimize_structure(total_rollouts, description=None, code_paths=None, verbose=False)</code>  <code>async</code>","text":"<p>Propose structural changes (i.e. changes that cannot be achieved by setting parameters alone) to improve the agent.</p> <p>Parameters:</p> Name Type Description Default <code>total_rollouts</code> <code>int</code> <p>Total number of rollouts to use for optimization. Generally, a moderate number of rollouts (e.g. 10-20) is required and recommended. For agents with longer execution traces: Try reducing the number of rollouts if an error is raised.</p> required <code>description</code> <code>str</code> <p>Text description of the current structure/workflow/... of the agent.</p> <code>None</code> <code>code_paths</code> <code>list[str]</code> <p>A list of paths corresponding to code files containing the implementation of the agent.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, additional information will be printed during the optimization. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Suggestion for structural changes to the agent.</p>"},{"location":"api/mockers.html","title":"Mockers","text":""},{"location":"api/mockers.html#relai.mocker.Persona","title":"<code>relai.mocker.Persona(user_persona, intent=None, starting_message=None, tools=None, model='gpt-5-mini')</code>","text":"<p>               Bases: <code>BaseMocker</code></p> <p>A mocker class for creating AI personas with specific behaviors and tools. A persona can be used to mimic a particular role by defining a system prompt and optionally equipping it with tools.</p> <p>Attributes:</p> Name Type Description <code>user_persona</code> <code>str</code> <p>The description of the persona's characteristics and behavior.</p> <code>intent</code> <code>str | None</code> <p>The intent or goal of the persona during interactions.</p> <code>starting_message</code> <code>str | None</code> <p>An optional initial message that the persona will use to start interactions.</p> <code>tools</code> <code>Optional[list[Tool]]</code> <p>A list of tools that the persona can use.</p> <p>Initializes the Persona with a description, intent, optional starting message, tools, and model.</p> <p>Parameters:</p> Name Type Description Default <code>user_persona</code> <code>str</code> <p>The description of the persona's characteristics and behavior.</p> required <code>intent</code> <code>str | None</code> <p>The intent or goal of the persona during interactions.</p> <code>None</code> <code>starting_message</code> <code>str | None</code> <p>An optional initial message that the persona will use to start interactions.</p> <code>None</code> <code>tools</code> <code>Optional[list[Tool]]</code> <p>A list of tools that the persona can use.</p> <code>None</code> <code>model</code> <code>str | None</code> <p>The AI model to use for simulating the persona's behavior.</p> <code>'gpt-5-mini'</code>"},{"location":"api/mockers.html#relai.mocker.MockTool","title":"<code>relai.mocker.MockTool(model='gpt-5-mini')</code>","text":"<p>               Bases: <code>BaseMocker</code></p> <p>A mocker class for simulating the behavior of a tool used by an AI agent.</p> <p>Initializes the MockTool with an optional model specification.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | None</code> <p>The AI model to use for simulating the tool's behavior.</p> <code>'gpt-5-mini'</code>"},{"location":"api/simulator.html","title":"Simulator","text":""},{"location":"api/simulator.html#relai.simulator.simulated","title":"<code>relai.simulator.simulated(func)</code>","text":"<p>Decorator to mark a function to be simulated using a mocker in simulation mode. All such functions must have a corresponding mocker set in the simulation configuration. Supports both synchronous and asynchronous functions.</p>"},{"location":"api/simulator.html#relai.simulator.EnvGenerator","title":"<code>relai.simulator.EnvGenerator</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for functions that generate simulation configurations. These functions take an optional RELAISample and return a SimulationConfigT which is a dictionary mapping qualified function names of functions decorated with <code>@simulated</code> to their respective mocker instances.</p>"},{"location":"api/simulator.html#relai.simulator.random_env_generator","title":"<code>relai.simulator.random_env_generator(config_set)</code>","text":"<p>An environment generator that uniformly samples a mocker for each simulated function from the provided set of mockers.</p> <p>Parameters:</p> Name Type Description Default <code>config_set</code> <code>dict[str, Sequence[BaseMocker]]</code> <p>A mapping from qualified function names to a sequence of possible mockers for that function.</p> required"},{"location":"api/simulator.html#relai.simulator.SyncSimulator","title":"<code>relai.simulator.SyncSimulator(agent_fn, env_generator=None, benchmark=None, log_runs=True, client=None)</code>","text":"<p>               Bases: <code>BaseSimulator</code></p> <p>A simulator for synchronous agent functions.</p> <p>Parameters:</p> Name Type Description Default <code>agent_fn</code> <code>SyncAgent</code> <p>The synchronous agent function to be simulated.</p> required <code>env_generator</code> <code>EnvGenerator | None</code> <p>An optional environment generator function. If not provided, a default generator that returns an empty configuration will be used. The default generator can only be used with simulations that don't require any mockers.</p> <code>None</code> <code>benchmark</code> <code>Benchmark | None</code> <p>An optional benchmark to source simulation samples from.</p> <code>None</code> <code>log_runs</code> <code>bool</code> <p>Whether to log the runs to the RELAI platform. Defaults to True.</p> <code>True</code> <code>client</code> <code>RELAI | None</code> <p>A synchronous RELAI client for logging. Must be provided if log_runs is True.</p> <code>None</code>"},{"location":"api/simulator.html#relai.simulator.SyncSimulator.rerun","title":"<code>rerun(simulation_tapes, group_id=None)</code>","text":"<p>Rerun the simulator for a list of simulation tapes.</p> <p>Parameters:</p> Name Type Description Default <code>simulation_tapes</code> <code>list[SimulationTape]</code> <p>The list of simulation tapes to rerun. This allows for re-executing the agent in an environment identical to a previous run and is useful for debugging and optimization.</p> required <code>group_id</code> <code>str</code> <p>An optional group ID to associate all runs together. If not provided, a new UUID will be generated.</p> <code>None</code>"},{"location":"api/simulator.html#relai.simulator.SyncSimulator.run","title":"<code>run(num_runs, group_id=None)</code>","text":"<p>Run the simulator for a specified number of times.</p> <p>Parameters:</p> Name Type Description Default <code>num_runs</code> <code>int</code> <p>The number of simulation runs to execute.</p> required <code>group_id</code> <code>str</code> <p>An optional group ID to associate all runs together. If not provided, a new UUID will be generated.</p> <code>None</code>"},{"location":"api/simulator.html#relai.simulator.AsyncSimulator","title":"<code>relai.simulator.AsyncSimulator(agent_fn, env_generator=None, benchmark=None, log_runs=True, client=None)</code>","text":"<p>               Bases: <code>BaseSimulator</code></p> <p>A simulator for asynchronous agent functions.</p> <p>Parameters:</p> Name Type Description Default <code>agent_fn</code> <code>AsyncAgent</code> <p>The asynchronous agent function to be simulated.</p> required <code>env_generator</code> <code>EnvGenerator | None</code> <p>An optional environment generator function. If not provided, a default generator that returns an empty configuration will be used. The default generator can only be used with simulations that don't require any mockers.</p> <code>None</code> <code>benchmark</code> <code>Benchmark | None</code> <p>An optional benchmark to source simulation samples from.</p> <code>None</code> <code>log_runs</code> <code>bool</code> <p>Whether to log the runs to the RELAI platform. Defaults to True.</p> <code>True</code> <code>client</code> <code>RELAI | None</code> <p>A asynchronous RELAI client for logging. Must be provided if log_runs is True.</p> <code>None</code>"},{"location":"api/simulator.html#relai.simulator.AsyncSimulator.rerun","title":"<code>rerun(simulation_tapes, group_id=None)</code>  <code>async</code>","text":"<p>Rerun the simulator for a list of simulation tapes.</p> <p>Parameters:</p> Name Type Description Default <code>simulation_tapes</code> <code>list[SimulationTape]</code> <p>The list of simulation tapes to rerun. This allows for re-executing the agent in an environment identical to a previous run and is useful for debugging and optimization.</p> required <code>group_id</code> <code>str</code> <p>An optional group ID to associate all runs together. If not provided, a new UUID will be generated.</p> <code>None</code>"},{"location":"api/simulator.html#relai.simulator.AsyncSimulator.run","title":"<code>run(num_runs, group_id=None)</code>  <code>async</code>","text":"<p>Run the simulator for a specified number of times.</p> <p>Parameters:</p> Name Type Description Default <code>num_runs</code> <code>int</code> <p>The number of simulation runs to execute.</p> required <code>group_id</code> <code>str</code> <p>An optional group ID to associate all runs together. If not provided, a new UUID will be generated.</p> <code>None</code>"},{"location":"api/types.html","title":"Types","text":""},{"location":"api/types.html#relai.data.RELAISample","title":"<code>relai.data.RELAISample(benchmark_id='default', id=(lambda: uuid4().hex)(), split='All', agent_inputs=dict(), extras=dict(), serialized_simulation_config=dict())</code>  <code>dataclass</code>","text":"<p>Represents a single sample in a RELAI benchmark.</p> <p>Attributes:</p> Name Type Description <code>benchmark_id</code> <code>str</code> <p>The identifier of the benchmark this sample belongs to.</p> <code>id</code> <code>str</code> <p>The unique identifier for this sample.</p> <code>split</code> <code>str</code> <p>The data split this sample belongs to (e.g., \"Train\", \"Validation\", \"Test\").</p> <code>agent_inputs</code> <code>AgentInputs</code> <p>The inputs provided to the agent from this sample.</p> <code>extras</code> <code>Extras</code> <p>Any additional metadata or information associated with this sample. Use this field to also store evaluator-specific inputs.</p> <code>serialized_simulation_config</code> <code>dict[str, Any]</code> <p>The serialized simulation configuration for this sample. Can be used to reconstruct any mockers used in a previous simulation.</p>"},{"location":"api/types.html#relai.data.SimulationTape","title":"<code>relai.data.SimulationTape(sample=None, id=(lambda: uuid4().hex)(), simulation_config=dict())</code>  <code>dataclass</code>","text":"<p>A simulation tape records any inputs, outputs, and any other relevant data during a simulation.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The unique identifier for this simulation tape.</p> <code>benchmark_id</code> <code>str</code> <p>The identifier of the benchmark from which the sample used to initialize this tape was taken. If no sample was provided, defaults to \"default\".</p> <code>sample_id</code> <code>str</code> <p>The identifier of the sample used to initialize this tape. If no sample was provided, defaults to the tape's id.</p> <code>split</code> <code>str</code> <p>The data split of the sample used to initialize this tape. If no sample was provided, defaults to \"All\".</p> <code>agent_inputs</code> <code>AgentInputs</code> <p>The inputs provided to the agent from the sample used to initialize this tape. If no sample was provided, defaults to an empty dictionary.</p> <code>extras</code> <code>Extras</code> <p>Any additional metadata or information associated with this tape. If no sample was provided, defaults to an empty dictionary.</p> <code>evaluator_group</code> <code>str</code> <p>The evaluator group associated with this tape, typically set to the benchmark_id of the sample. If no sample was provided, defaults to \"default\". Can be modified in the <code>agent_fn</code>.</p> <code>simulation_config</code> <code>SimulationConfigT</code> <p>The simulation configuration used during this simulation which is a mapping from qualified function names to their respective mocker instances (Persona, MockTool etc.,).</p>"},{"location":"api/types.html#relai.data.AgentLog","title":"<code>relai.data.AgentLog(simulation_tape, agent_outputs=dict(), trace_id=None)</code>  <code>dataclass</code>","text":"<p>Log of a single agent simulation run.</p> <p>Attributes:</p> Name Type Description <code>simulation_tape</code> <code>SimulationTape</code> <p>The simulation tape containing inputs and metadata.</p> <code>agent_outputs</code> <code>AgentOutputs</code> <p>The outputs generated by the agent during the simulation.</p> <code>trace_id</code> <code>str | None</code> <p>An optional trace identifier for the simulation run.</p>"},{"location":"api/types.html#relai.data.EvaluatorLog","title":"<code>relai.data.EvaluatorLog(evaluator_id, name, outputs, config=dict())</code>  <code>dataclass</code>","text":"<p>Log of a single evaluator run.</p> <p>Attributes:</p> Name Type Description <code>evaluator_id</code> <code>str</code> <p>The ID of the evaluator.</p> <code>name</code> <code>str</code> <p>The name of the evaluator.</p> <code>outputs</code> <code>EvaluatorOutputs</code> <p>The outputs generated by the evaluator.</p> <code>config</code> <code>dict[str, Any]</code> <p>The configuration settings used for the evaluator.</p>"},{"location":"api/types.html#relai.data.CriticoLog","title":"<code>relai.data.CriticoLog(agent_log, evaluator_logs=list(), aggregate_score=0.0, aggregate_feedback='', trace_id=None)</code>  <code>dataclass</code>","text":"<p>Log of a Critico evaluation run.</p> <p>Attributes:</p> Name Type Description <code>agent_log</code> <code>AgentLog</code> <p>The log of the agent simulation run.</p> <code>evaluator_logs</code> <code>list[EvaluatorLog]</code> <p>A list of logs from individual evaluators.</p> <code>aggregate_score</code> <code>float</code> <p>The aggregate score computed from all the evaluator logs.</p> <code>aggregate_feedback</code> <code>str</code> <p>The aggregate feedback compiled from all the evaluator logs.</p> <code>trace_id</code> <code>str | None</code> <p>An optional trace identifier for the corresponding agent simulation run.</p>"},{"location":"tutorials/index.html","title":"Index","text":"Tutorials <ul> <li>Agent Annotation Benchmark: Turn annotated simulation runs into reusable benchmarks that drive Maestro config and structure optimization.</li> <li>Persona Set: Curate and deploy persona collections, then bind them to simulated entry points for richer user context.</li> <li>Mock MCP Server: Spin up platform-hosted mock MCP servers so agents can exercise tool calls without hitting production services.</li> <li>Custom Evaluator: Implement a bespoke <code>Evaluator</code> subclass that scores agent outputs with custom logic and metadata requirements.</li> </ul>"},{"location":"tutorials/annotation-benchmark.html","title":"Annotation Benchmark","text":"Agent Annotation Benchmark <p>Annotation benchmarks are benchmarks created by annotating (providing feedback to) runs of agents.  They can be used directly in agent optimization (configs, structure).  For a detailed example of how to run agents in a simulated environment and  how to use annotation benchmarks in agent optimization, see simulate-&gt;annotate-&gt;optimze.</p> Create Annotation Benchmark <ol> <li> <p>To create an annotation benchmark, first go to RELAI platform and find Run under Results.</p> <p> <li> <p>Click on individual runs to inspect any agent you executed in a simulated environment.</p> <p></p> </li> <li> <p>Annotate the runs with the <code>Like/Dislike</code>, <code>Desired Output</code>, <code>Feedback</code> fields and save your changes.</p> <p></p> </li> <li> <p>Use the \"Add to Benchmark\" button at the bottom to add the annotated run as a sample to the benchmark of your choice.  (Use the <code>Create a new annoatation benchmark</code> function if you have not created any benchmark yet)</p> <p></p> </li> <li> <p>Continue to annotate and add other runs to the benchmark. The benchmark is already ready-to-use with its benchmark id. See simulate-&gt;annotate-&gt;optimze for how to use annotation benchmarks in  agent optimization.</p> </li>"},{"location":"tutorials/custom-evaluator.html","title":"Custom Evaluator","text":"Custom Evaluator <p>To define a custom evaluator, write a class basing <code>relai.critico.evaluate.Evaluator</code> and override <code>compute_evaluator_result</code> to describe your custom evaluation logic.</p> <pre><code>from collections.abc import Callable\nfrom typing import override\n\nfrom relai import AgentLog, EvaluatorLog\nfrom relai.critico.evaluate import Evaluator\n\n\nclass CustomSentimentEvaluator(Evaluator):\n    \"\"\"\n    A custom evaluator for sentiment analysis tasks.\n\n    This evaluator compares the agent's predicted sentiment against the ground truth\n    and provides detailed scoring based on prediction accuracy and confidence.\n    \"\"\"\n\n    def __init__(\n        self,\n        transform: Callable | None = None,\n        correct_score: float = 1.0,\n        incorrect_score: float = 0.0,\n        partial_credit: bool = True,\n    ):\n        \"\"\"\n        Initialize the custom sentiment evaluator.\n\n        Args:\n            transform: Optional function to transform agent outputs\n            correct_score: Score to assign for correct predictions\n            incorrect_score: Score to assign for incorrect predictions\n            partial_credit: Whether to give partial credit for neutral predictions\n        \"\"\"\n        super().__init__(\n            name=\"custom-sentiment-evaluator\",\n            # Specify required fields from the benchmark and agent response\n            required_fields=[\"text\", \"predicted_sentiment\", \"true_sentiment\"],\n            transform=transform,\n            # Store configuration as hyperparameters\n            correct_score=correct_score,\n            incorrect_score=incorrect_score,\n            partial_credit=partial_credit,\n        )\n\n    @override\n    async def compute_evaluator_result(self, agent_log: AgentLog) -&gt; EvaluatorLog:\n        \"\"\"\n        Evaluate the agent's sentiment prediction against ground truth.\n\n        Args:\n            agent_log (AgentLog): The response from the AI agent, containing the original sample\n                and agent outputs.\n\n        Returns:\n            EvaluatorLog: Evaluator log with score and feedback\n        \"\"\"\n        # Extract required fields from different sources\n        text = agent_log.simulation_tape.agent_inputs[\"text\"]\n        predicted_sentiment = agent_log.agent_outputs[\"predicted_sentiment\"]\n        true_sentiment = agent_log.simulation_tape.extras[\"true_sentiment\"]\n\n        # Evaluate prediction accuracy\n        if predicted_sentiment.lower() == true_sentiment.lower():\n            score = self.hyperparameters[\"correct_score\"]\n            feedback = f\"Correct! Predicted '{predicted_sentiment}' matches true sentiment '{true_sentiment}'\"\n        elif (\n            self.hyperparameters[\"partial_credit\"]\n            and predicted_sentiment.lower() == \"neutral\"\n            and true_sentiment.lower() in [\"positive\", \"negative\"]\n        ):\n            # Give partial credit for neutral predictions on polar sentiments\n            score = self.hyperparameters[\"correct_score\"] * 0.5\n            feedback = f\"Partial credit: Predicted neutral for {true_sentiment} sentiment\"\n        else:\n            score = self.hyperparameters[\"incorrect_score\"]\n            feedback = f\"Incorrect: Predicted '{predicted_sentiment}' but true sentiment is '{true_sentiment}'\"\n\n        # Add text length as additional context\n        text_length = len(text.split())\n        feedback += f\" (Text length: {text_length} words)\"\n\n        return EvaluatorLog(\n            evaluator_id=self.uid,\n            name=self.name,\n            outputs={\"score\": score, \"feedback\": feedback},\n        )\n</code></pre>"},{"location":"tutorials/mock-mcp-server.html","title":"Mock MCP Server","text":"Mock MCP Server <p>You can use a mock MCP server to mimic a real MCP server. Mock MCP servers are useful during agent development, simulation and testing.</p> Create Mock Server <ol> <li> <p>First add the tools you want to mock on the Mock Tools page:</p> <p> <li> <p>You can either enter the input/output schema manually or import that information directly from an existing MCP server</p> <p> <li> <p>Once the tools you want to mock are added, you pick the tools you want to include for mocking and can create a Mock MCP Server:</p> <p> <li> <p>You can deploy the server by clicking on the new Mock MCP server entry and then on \"Deploy\"</p> <p> <li> <p>You can use this mock MCP server anywhere you use a real MCP server (with agents, LLM tool call etc.,).</p> </li>"},{"location":"tutorials/persona-set.html","title":"Persona Set","text":"Persona Set <p>An AI persona can be used to mimic a particular role by defining a system prompt and optionally equipping it with tools.  A persona set is set of personas, which can be conveniently curated and/or imported through RELAI platform.</p> Create Persona Set <ol> <li> <p>To create a persona set, first go to RELAI platform and find Persona Sets under AgentHub.</p> <p> <li> <p>Click \"Create Persona Set\".</p> <p></p> </li> <li> <p>Name the new persona set; Upload a CSV file with a <code>system_prompts</code> column to populate the list, or add prompts manually.</p> <p></p> </li> <li> <p>And done! Your persona set is created. You can chat with personas directly here and adjust them further whenever needed.</p> <p></p> </li> Use Persona Set in Simulation <ol> <li> <p>Decorate inputs/tools that will be simulated.</p> <pre><code>from relai import simulated\n\n@simulated\nasync def get_user_input():\n    msg = input(\"User: \")\n    return msg\n</code></pre> </li> <li> <p>When setting up the simulation environment, bind the persona set to the corresponding fully-qualified function names.</p> <pre><code>from relai.simulator import AsyncSimulator, random_env_generator\n\nenv_generator = random_env_generator(\n    {\"__main__.get_user_input\": PersonaSet(persona_set_id=\"your_persona_set_id_here\")}\n)\n\nasync def main():\n    async with AsyncRELAI() as client:\n        simulator = AsyncSimulator(\n            client=client,\n            agent_fn=&lt;your agent function here&gt;, \n            env_generator=env_generator,\n            log_runs=True,\n        )\n\n        agent_logs = await simulator.run(num_runs=4)\n        print(agent_logs)\n\nasyncio.run(main())\n</code></pre> </li> </ol>"}]}