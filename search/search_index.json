{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"RELAI: Simulate \u2192 Evaluate \u2192 Optimize AI Agents <p>RELAI is an SDK for building reliable AI agents. It streamlines the hardest parts of agent development\u2014simulation, evaluation, and optimization\u2014so you can iterate quickly with confidence.</p> <p>What you get</p> <ul> <li> <p>Agent Simulation \u2014 Create full/partial environments, define LLM personas, mock MCP servers &amp; tools, and generate synthetic data. Optionally condition simulation on real samples to better match production.</p> </li> <li> <p>Agent Evaluation \u2014 Mix code-based and LLM-based custom evaluators or use RELAI platform evaluators. Turn human reviews into benchmarks you can re-run.</p> </li> <li> <p>Agent Optimization (Maestro) \u2014 Holistic optimizer that uses evaluator signals &amp; feedback to improve prompts/configs and suggest graph-level changes. Also selects best model/tool/graph based on observed performance.</p> </li> </ul>"},{"location":"index.html#quick-links","title":"Quick Links","text":"<p>Get up and running with RELAI in minutes:</p> <ul> <li>Getting Started - Installation, setup and code walkthrough</li> <li>Tutorials</li> <li>Examples - Self-contained examples illustrating using the SDK in various common scenarios</li> </ul>"},{"location":"benchmark.html","title":"Benchmark","text":""},{"location":"benchmark.html#relai.benchmark.Benchmark","title":"<code>relai.benchmark.Benchmark(benchmark_id, samples=None)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for defining and managing benchmarks.</p> <p>This class provides a foundational structure for benchmarks, enabling the download, and iteration of samples. It ensures that all concrete benchmark implementations have a unique identifier and a collection of samples to be used as inputs for AI agents and evaluators.</p> <p>Attributes:</p> Name Type Description <code>benchmark_id</code> <code>str</code> <p>A unique identifier for this specific benchmark.</p> <code>samples</code> <code>list[RELAISample]</code> <p>A list of <code>RELAISample</code> objects contained within this benchmark. Defaults to an empty list if not provided.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_id</code> <code>str</code> <p>The unique identifier for the benchmark.</p> required <code>samples</code> <code>list[RELAISample]</code> <p>A list of <code>RELAISample</code> objects to include in the benchmark. Defaults to an empty list.</p> <code>None</code>"},{"location":"benchmark.html#relai.benchmark.Benchmark.__iter__","title":"<code>__iter__()</code>","text":"<p>Enables iteration over the samples within the benchmark as follows:</p> <pre><code>for sample in benchmark:\n    # Process each sample\n    pass\n</code></pre> <p>Yields:</p> Name Type Description <code>RELAISample</code> <code>RELAISample</code> <p>Each <code>RELAISample</code> object contained in the benchmark.</p>"},{"location":"benchmark.html#relai.benchmark.Benchmark.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of samples currently in the benchmark.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The total count of <code>Sample</code> objects.</p>"},{"location":"benchmark.html#relai.benchmark.Benchmark.sample","title":"<code>sample(n=1)</code>","text":"<p>Returns <code>n</code> random samples from the benchmark, with replacement.</p> <p>If <code>n</code> is greater than the total number of samples, samples may be repeated in the returned list.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of random samples to retrieve. Must be a positive integer. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[RELAISample]</code> <p>list[RELAISample]: A list containing <code>n</code> randomly selected <code>RELAISample</code> objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>n</code> is less than or equal to 0.</p>"},{"location":"benchmark.html#relai.benchmark.RELAIBenchmark","title":"<code>relai.benchmark.RELAIBenchmark(benchmark_id, field_name_mapping=None, field_value_transform=None, agent_input_fields=None, extra_fields=None)</code>","text":"<p>               Bases: <code>Benchmark</code></p> <p>A concrete implementation of <code>Benchmark</code> that downloads samples from the RELAI platform.</p> <p>Attributes:</p> Name Type Description <code>benchmark_id</code> <code>str</code> <p>The unique identifier (ID) of the RELAI benchmark to be loaded from the platform. You can find the benchmark ID in the metadata of the benchmark.</p> <code>samples</code> <code>list[RELAISample]</code> <p>A list of <code>RELAISample</code> objects contained within this benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_id</code> <code>str</code> <p>The unique identifier for the RELAI benchmark. This ID is used to fetch the benchmark data from the RELAI platform.</p> required <code>field_name_mapping</code> <code>dict[str, str]</code> <p>A mapping from field names returned by the RELAI API to standardized field names expected by the evaluators. If a field name is not present in this mapping, it is used as-is. Defaults to an empty dictionary.</p> <code>None</code> <code>field_value_transform</code> <code>dict[str, Callable]</code> <p>A mapping from field names to transformation functions that convert field values from the RELAI API into the desired format. If a field name is not present in this mapping, the identity function is used (i.e., no transformation). Defaults to an empty dictionary.</p> <code>None</code> <code>agent_input_fields</code> <code>list[str]</code> <p>A list of field names to extract from each sample for the <code>agent_inputs</code> dictionary. These fields are provided to the AI agent. Defaults to an empty list.</p> <code>None</code> <code>extra_fields</code> <code>list[str]</code> <p>A list of field names to extract from each sample for the <code>extras</code> dictionary. These fields are also provided to the evaluators. Defaults to an empty list.</p> <code>None</code>"},{"location":"benchmark.html#relai.benchmark.RELAIBenchmark.fetch_samples","title":"<code>fetch_samples()</code>","text":"<p>Downloads samples from the RELAI platform and populates the <code>samples</code> attribute.</p> <p>This method fetches the benchmark data using the RELAI client and processes each sample to create <code>Sample</code> objects. The <code>samples</code> attribute is then updated with the newly fetched samples.</p>"},{"location":"benchmark.html#relai.benchmark.RELAIQuestionAnsweringBenchmark","title":"<code>relai.benchmark.RELAIQuestionAnsweringBenchmark(benchmark_id)</code>","text":"<p>               Bases: <code>RELAIBenchmark</code></p> <p>A concrete implementation of <code>RELAIBenchmark</code> for question-answering tasks. All samples in this benchmark have the following fields:</p> <ul> <li><code>agent_inputs</code>:<ul> <li><code>question</code>: The question to be answered by the AI agent.</li> </ul> </li> <li><code>extras</code>:<ul> <li><code>rubrics</code>:  A dictionary of rubrics for evaluating the answer.</li> <li><code>std_answer</code>: The standard answer to the question. }</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>benchmark_id</code> <code>str</code> <p>The unique identifier for the RELAI question-answering benchmark. This ID is used to fetch the benchmark data from the RELAI platform.</p> required"},{"location":"benchmark.html#relai.benchmark.RELAISummarizationBenchmark","title":"<code>relai.benchmark.RELAISummarizationBenchmark(benchmark_id)</code>","text":"<p>               Bases: <code>RELAIBenchmark</code></p> <p>A concrete implementation of <code>RELAIBenchmark</code> for summarization tasks. All samples in this benchmark have the following fields:</p> <ul> <li><code>agent_inputs</code>:<ul> <li><code>source</code>: The text to be summarized.</li> </ul> </li> <li><code>extras</code>:<ul> <li><code>key_facts</code>: A list of key facts extracted from the source.</li> <li><code>style_rubrics</code>: A dictionary of rubrics for evaluating the style of the summary.</li> <li><code>format_rubrics</code>: A dictionary of rubrics for evaluating the format of the summary.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>benchmark_id</code> <code>str</code> <p>The unique identifier for the RELAI summarization benchmark. This ID is used to fetch the benchmark data from the RELAI platform.</p> required"},{"location":"benchmark.html#relai.benchmark.CSVBenchmark","title":"<code>relai.benchmark.CSVBenchmark(csv_file, agent_input_columns=None, extra_columns=None, benchmark_id=None)</code>","text":"<p>               Bases: <code>Benchmark</code></p> <p>A concrete implementation of <code>Benchmark</code> that loads samples from a CSV file.</p> <p>Attributes:</p> Name Type Description <code>benchmark_id</code> <code>str</code> <p>The unique identifier (ID) of the benchmark to be loaded from the CSV file. Defaults to the CSV file name.</p> <code>samples</code> <code>list[Sample]</code> <p>A list of <code>Sample</code> objects contained within this benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>The path to the CSV file containing benchmark samples.</p> required <code>agent_input_columns</code> <code>list[str]</code> <p>A list of column names in the CSV file that should be used as inputs for the AI agent. Defaults to an empty list.</p> <code>None</code> <code>extra_columns</code> <code>list[str]</code> <p>A list of column names in the CSV file that could be used as inputs for evaluators. Defaults to an empty list.</p> <code>None</code> <code>benchmark_id</code> <code>str</code> <p>A unique identifier for the benchmark. If not provided, it defaults to the name of the CSV file.</p> <code>None</code>"},{"location":"critico.html","title":"Critico","text":""},{"location":"critico.html#relai.critico.Critico","title":"<code>relai.critico.Critico(client)</code>","text":"<p>Critico orchestrates evaluation of an AI agent using a configurable set of evaluators.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required"},{"location":"critico.html#relai.critico.Critico.add_evaluators","title":"<code>add_evaluators(evaluators, evaluator_group='default')</code>","text":"<p>Adds a new evaluator to Critico to a specific evaluator group.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator_group</code> <code>str</code> <p>The name of the evaluator group.</p> <code>'default'</code> <code>evaluators</code> <code>dict[Evaluator, float]</code> <p>A dictionary where keys are <code>Evaluator</code> objects and values are their corresponding weights for this evaluator group. If <code>None</code>, no evaluators are associated with this evaluator group initially. Defaults to <code>None</code>.</p> required"},{"location":"critico.html#relai.critico.Critico.evaluate","title":"<code>evaluate(agent_logs)</code>  <code>async</code>","text":"<p>Evaluates a list of AI agent logs against their corresponding evaluator groups using the configured evaluators.</p> <p>For each <code>AgentLog</code>, Critico identifies the associated evaluator group and runs all evaluators configured for that group. It then aggregates the individual evaluator results (scores and feedback) into a single <code>CriticoFeedback</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>agent_logs</code> <code>list[AgentLog]</code> <p>A list of <code>AgentLog</code> objects to be evaluated. Each log must contain a <code>simulation_tape.evaluator_group</code> that corresponds to an evaluator group added to Critico.</p> required <p>Returns:</p> Type Description <code>list[CriticoLog]</code> <p>list[CriticoFeedback]: A list of <code>CriticoFeedback</code> objects, where each object summarizes the evaluation outcome for one <code>AgentLog</code>.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a <code>evaluator_group</code> within an <code>AgentLog</code> is not found       among Critico's managed evaluator groups.</p>"},{"location":"critico.html#relai.critico.Critico.report","title":"<code>report(critico_logs)</code>  <code>async</code>","text":"<p>Submits the critico logs (as multiple <code>CriticoLog</code> objects) to the RELAI platform.</p> <p>Parameters:</p> Name Type Description Default <code>critico_logs</code> <code>list[CriticoLog]</code> <p>A list of <code>CriticoLog</code> objects containing the evaluation results for each <code>AgentLog</code>.</p> required <p>Raises:</p> Type Description <code>RELAIError</code> <p>If any <code>CriticoLog</code> does not have a valid <code>trace_id</code>.</p>"},{"location":"critico.html#relai.critico.Critico.reweight_evaluator","title":"<code>reweight_evaluator(evaluator_group, evaluator, weight)</code>","text":"<p>Adjusts the weight of a specific evaluator in a given evaluator group.</p> <p>Evaluator weights influence their contribution to the aggregate score during the <code>evaluate()</code> operation.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator_group</code> <code>str</code> <p>The name of the evaluator group.</p> required <code>evaluator</code> <code>Evaluator</code> <p>The <code>Evaluator</code> object whose weight is to be adjusted.</p> required <code>weight</code> <code>float</code> <p>The new weight (a positive float) for the evaluator. Higher weights give more prominence to its results in aggregation.</p> required <p>Raises:</p> Type Description <code>RELAIError</code> <p>If the <code>evaluator_group</code> is not found in Critico, or if the <code>evaluator</code> is not associated with the specified <code>evaluator_group</code>.</p> <code>ValueError</code> <p>If the specified <code>weight</code> is not a positive float.</p>"},{"location":"evaluator.html","title":"Evaluator","text":""},{"location":"evaluator.html#relai.critico.evaluate.Evaluator","title":"<code>relai.critico.evaluate.Evaluator(name, required_fields=None, transform=None, **hyperparameters)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for defining and implementing evaluators for a benchmark.</p> <p>Evaluators are responsible for assessing an AI agent's response to a specific benchmark sample. They can define required input fields from the <code>AgentLog</code> necessary for their evaluation logic and may incorporate customizable hyperparameters to tune their behavior.</p> <p>Subclasses must implement the <code>compute_evaluator_result</code> method to define their specific evaluation logic, which produces an <code>EvaluatorResponse</code> object.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the evaluator, used for identification.</p> <code>required_fields</code> <code>list[str]</code> <p>A list of field names (keys) that must be present in either <code>agent_inputs</code> (of the sample), <code>eval_inputs</code> (of the sample), or <code>agent_outputs</code> (of the agent response).</p> <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>hyperparameters</code> <code>dict[str, Any]</code> <p>A dictionary of arbitrary keyword arguments passed during initialization, allowing for custom configuration of the evaluator's behavior.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The display name of the evaluator, used to identify the evaluator in evaluation results.</p> required <code>required_fields</code> <code>list[str]</code> <p>A list of field names (keys) that must be present in either <code>agent_inputs</code> (of the sample), <code>eval_inputs</code> (of the sample), or <code>agent_outputs</code> (of the agent response).</p> <code>None</code> <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code> <code>hyperparameters</code> <code>dict[str, Any]</code> <p>A dictionary of arbitrary keyword arguments passed during initialization, allowing for custom configuration of the evaluator's behavior.</p> <code>{}</code>"},{"location":"evaluator.html#relai.critico.evaluate.Evaluator.uid","title":"<code>uid</code>  <code>cached</code> <code>property</code>","text":"<p>Generates a unique identifier for this specific evaluator instance. The UID is constructed from the evaluator's class name combined with a JSON-serialized representation of its hyperparameters.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A unique identifier for the evaluator.</p>"},{"location":"evaluator.html#relai.critico.evaluate.Evaluator.__call__","title":"<code>__call__(agent_log)</code>  <code>async</code>","text":"<p>Executes the evaluation process for a given AI agent response.</p> <p>Parameters:</p> Name Type Description Default <code>agent_log</code> <code>AgentLog</code> <p>The response from the AI agent to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>EvaluatorResponse</code> <code>EvaluatorLog</code> <p>The structured result of the evaluation, including any computed <code>score</code> and <code>feedback</code>, as defined by the concrete evaluator.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>agent_log</code> is not an instance of <code>AgentLog</code> or <code>agent_outputs</code> (after transform) in agent_log is not a dict.</p> <code>ValueError</code> <p>If any <code>required_fields</code> are missing from the <code>agent_log</code>.</p>"},{"location":"evaluator.html#relai.critico.evaluate.Evaluator.__hash__","title":"<code>__hash__()</code>","text":"<p>Computes the hash value for the evaluator based on its unique identifier (<code>uid</code>).</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The hash value of the evaluator's unique identifier.</p>"},{"location":"evaluator.html#relai.critico.evaluate.Evaluator.compute_evaluator_result","title":"<code>compute_evaluator_result(agent_log)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Abstract method: Computes the evaluation result for an agent response.</p> <p>Concrete subclasses must implement this method to define their unique evaluation logic. This method should process the <code>AgentLog</code> by accessing its <code>sample</code> and <code>agent_outputs</code> to derive the evaluation outcome, which is then encapsulated in an <code>EvaluatorResponse</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>agent_log</code> <code>AgentLog</code> <p>The comprehensive response from the AI agent, including the original sample and agent's outputs.</p> required <p>Returns:</p> Name Type Description <code>EvaluatorResponse</code> <code>EvaluatorLog</code> <p>An instance of <code>EvaluatorResponse</code> containing the evaluation outcome, typically including a <code>score</code> and/or <code>feedback</code>, along with <code>evaluator_name</code>, <code>evaluator_configuration</code>, and the original <code>agent_log</code>.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If a concrete subclass does not override this method.</p>"},{"location":"evaluator.html#relai.critico.evaluate.RELAIEvaluator","title":"<code>relai.critico.evaluate.RELAIEvaluator(client, relai_evaluator_name, name, required_fields=None, transform=None, **hyperparameters)</code>","text":"<p>               Bases: <code>Evaluator</code></p> <p>Base class for all RELAI evaluators that use the RELAI API to evaluate responses on a benchmark.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the specific evaluator to be invoked on the RELAI platform.</p> <p>Initializes a new RELAIEvaluator instance.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>relai_evaluator_name</code> <code>str</code> <p>The name of the RELAI evaluator to be used for evaluation.</p> required <code>name</code> <code>str</code> <p>The display name of the evaluator, used to identify the evaluator in evaluation results.</p> required <code>required_fields</code> <code>list[str]</code> <p>A list of field names that must be present in the <code>AgentLog</code> (across agent inputs, eval inputs, or agent outputs). Defaults to an empty list.</p> <code>None</code> <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code> <code>**hyperparameters</code> <code>Any</code> <p>Arbitrary keyword arguments passed to the base <code>Evaluator</code> class and also forwarded to the RELAI evaluator.</p> <code>{}</code>"},{"location":"evaluator.html#relai.critico.evaluate.RELAIEvaluator.compute_evaluator_result","title":"<code>compute_evaluator_result(agent_log)</code>  <code>async</code>","text":"<p>Computes the structured evaluation result by invoking a RELAI evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>agent_log</code> <code>AgentLog</code> <p>The response from the AI agent.</p> required <p>Returns:</p> Name Type Description <code>EvaluatorResponse</code> <code>EvaluatorLog</code> <p>A structured evaluation result containing the evaluator's unique ID, the original agent response, and an optional <code>score</code> and <code>feedback</code> computed by the RELAI evaluator.</p>"},{"location":"evaluator.html#relai.critico.evaluate.RELAILengthEvaluator","title":"<code>relai.critico.evaluate.RELAILengthEvaluator(client, measure='words', use_ratio=False, acceptable_range=None, target_ratio=None, slope=1.0, temperature=1.0, transform=None)</code>","text":"<p>               Bases: <code>RELAIEvaluator</code></p> <p>Evaluator to assess the length of generated text (e.g., summaries) using a RELAI evaluator. Supports evaluating length in as measured by number of characters, words, or sentences, or based on the compression ratio.</p> <p>Required fields:</p> <pre><code>- `source`: The original text or document from which the summary is derived.\n- `summary`: The generated summary to be evaluated.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>measure</code> <code>str</code> <p>The unit for length calculation; one of: - 'characters': count every character, - 'words': split on whitespace, - 'sentences': split on sentence-ending punctuation (., !, ?). Defaults to 'words'.</p> <code>'words'</code> <code>use_ratio</code> <code>bool</code> <p>If True, ignore <code>acceptable_range</code> and instead evaluate the length based on the compression ratio: <code>1 - (summary_length / source_length)</code> relative to <code>target_ratio</code>. Defaults to False.</p> <code>False</code> <code>acceptable_range</code> <code>tuple[int, int]</code> <p>A two-element tuple <code>(min_len, max_len)</code> specifying the inclusive bounds for the length of <code>summary</code> under the chosen <code>measure</code>. Required if <code>use_ratio</code> is False. Ignored if <code>use_ratio</code> is True. Defaults to None.</p> <code>None</code> <code>target_ratio</code> <code>float</code> <p>The desired summary-to-source length ratio (between 0 and 1). Required if <code>use_ratio</code> is True. Defaults to None.</p> <code>None</code> <code>slope</code> <code>float</code> <p>A factor in [0, 1] controlling the penalty slope for summaries shorter than the lower bound. A slope of 1.0 yields a linear ramp from 0 at zero length to 1.0 at <code>min_len</code>. Defaults to 1.0.</p> <code>1.0</code> <code>temperature</code> <code>float</code> <p>A positive scaling factor that smooths the exponential penalty for summaries exceeding the upper bound. Higher values make the penalty curve flatter. Defaults to 1.0.</p> <code>1.0</code> <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the parameters are invalid.</p>"},{"location":"evaluator.html#relai.critico.evaluate.RELAIContentEvaluator","title":"<code>relai.critico.evaluate.RELAIContentEvaluator(client, transform=None)</code>","text":"<p>               Bases: <code>RELAIEvaluator</code></p> <p>Evaluator for assessing the factual content of a generated summary against provided key facts, using a RELAI evaluator.</p> <p>Required fields:</p> <pre><code>- `key_facts`: A dictionary of key facts with their associated weights, which the summary should cover.\n- `summary`: The generated summary to be evaluated.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code>"},{"location":"evaluator.html#relai.critico.evaluate.RELAIHallucinationEvaluator","title":"<code>relai.critico.evaluate.RELAIHallucinationEvaluator(client, transform=None)</code>","text":"<p>               Bases: <code>RELAIEvaluator</code></p> <p>Evaluator for detecting factual inconsistencies or \"hallucinations\" in generated text (e.g., summaries) relative to a source document, using a RELAI evaluator.</p> <p>Required fields:</p> <pre><code>- `source`: The original text or document from which the summary is derived.\n- `summary`: The generated summary to be evaluated.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code>"},{"location":"evaluator.html#relai.critico.evaluate.RELAIStyleEvaluator","title":"<code>relai.critico.evaluate.RELAIStyleEvaluator(client, transform=None)</code>","text":"<p>               Bases: <code>RELAIEvaluator</code></p> <p>Evaluator for assessing the stylistic adherence of a generated summary based on provided rubrics, using a RELAI evaluator.</p> <p>Required fields:</p> <pre><code>- `style_rubrics`: A dictionary of style rubrics with their associated weights,\n    which the summary should adhere to.\n- `summary`: The generated summary to be evaluated.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code>"},{"location":"evaluator.html#relai.critico.evaluate.RELAIFormatEvaluator","title":"<code>relai.critico.evaluate.RELAIFormatEvaluator(client, transform=None)</code>","text":"<p>               Bases: <code>RELAIEvaluator</code></p> <p>Evaluator for assessing the formatting adherence of a generated summary based on provided rubrics, using a RELAI evaluator.</p> <p>Required fields:</p> <pre><code>- `format_rubrics`: A dictionary of format rubrics with their associated weights,\n    which the summary should adhere to.\n- `summary`: The generated summary to be evaluated.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code>"},{"location":"evaluator.html#relai.critico.evaluate.RELAIRubricBasedEvaluator","title":"<code>relai.critico.evaluate.RELAIRubricBasedEvaluator(client, transform=None)</code>","text":"<p>               Bases: <code>RELAIEvaluator</code></p> <p>Evaluator for performing a detailed, rubric-driven assessment of an AI agent's response to a query using an LLM-based evaluator on the RELAI platform.</p> <p>Required fields:</p> <pre><code>- `question`: The question or prompt that the AI agent was asked to respond to.\n- `answer`: The AI agent's generated response to the question.\n- `rubrics`: A dictionary of evaluation criteria with their associated weights,\n    which the answer should satisfy.\n- `std_answer`: The standard or expected answer against which the AI agent's response is evaluated.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>transform</code> <code>Callable</code> <p>An optional callable to transform (pre-process) the <code>agent_outputs</code> of the agent response for the evaluator. Defaults to None.</p> <code>None</code>"},{"location":"getting-started.html","title":"Getting Started","text":"Installation <p>You can install the RELAI SDK with using your favorite Python package manager (requires Python 3.9+):</p> <pre><code>pip install relai\n# or\nuv add relai\n</code></pre> Setting the RELAI API key <p>A RELAI API key is necessary to use features from the RELAI platform. You can get a RELAI API key from your RELAI enterprise dashboard. After you copy the key, assign to <code>RELAI_API_KEY</code> environment variable:</p> <pre><code>export RELAI_API_KEY=\"relai-...\"\n</code></pre> Building Reliable AI Agents with RELAI SDK Step 1 \u2014 Decorate inputs/tools that will be simulated <pre><code>from relai.mocker import Persona, MockTool\nfrom relai.simulator import simulated\nfrom agents import function_tool\n\nAGENT_NAME = \"Stock Chatbot\"\nMODEL = \"gpt-5-mini\"\n\n\n# Decorate functions to be mocked in the simulation\n@simulated\nasync def get_user_query() -&gt; str:\n    \"\"\"Get user's query about stock prices.\"\"\"\n    return \"What is the current price of AAPL stock?\"\n\n\n@function_tool\n@simulated\nasync def retriever(query: str) -&gt; list[str]:\n    \"\"\"\n    A retriever tool that returns relevant financial data for a given query about stock prices.\n    \"\"\"\n    return []\n</code></pre> Step 2 \u2014 Register params to be optimized and define your agent <pre><code>from agents import Agent, Runner\nfrom relai.maestro import params, register_param\n\nregister_param(\n    \"prompt\",\n    type=\"prompt\",\n    init_value=\"You are a helpful assistant for stock price questions.\",\n    desc=\"system prompt for the agent\",\n)\n\nasync def stock_price_chatbot(question: str) -&gt; dict[str, str]:\n    agent = Agent(\n        name=AGENT_NAME,\n        instructions=params.prompt,  # access registered parameter\n        model=MODEL,\n        tools=[retriever],\n    )\n    result = await Runner.run(agent, question)\n    return {\"answer\": result.final_output}\n</code></pre> Step 3 \u2014 Wrap agent for simulation traces <pre><code>from relai import AgentOutputs, SimulationTape\n\nasync def agent_fn(tape: SimulationTape) -&gt; AgentOutputs:\n    question = await get_user_query()\n    tape.agent_inputs[\"question\"] = question  # trace inputs for later auditing\n    return await stock_price_chatbot(question)\n</code></pre> Step 4 - Define evaluators <pre><code>import re\nfrom relai import AgentLog, EvaluatorLog\nfrom relai.critico.evaluate import Evaluator\n\nclass PriceFormatEvaluator(Evaluator):\n    \"\"\"Checks for correct price formats ($\u2026 with exactly two decimals).\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(name=\"PriceFormatEvaluator\", required_fields=[\"answer\"])\n\n    async def compute_evaluator_result(self, agent_log: AgentLog) -&gt; EvaluatorLog:\n        # flag $-prices that are NOT like $1,234.56 or $1234.56\n        bad_pattern = r\"\\$(?!\\d{1,3}(?:,\\d{3})+|\\d+\\.\\d{2}\\b)\\S+\"\n        bad_prices = re.findall(bad_pattern, agent_log.agent_outputs[\"answer\"])\n        score = 0.0 if bad_prices else 1.0\n        feedback = (\n            (\"Incorrect price formats found: \" + \", \".join(bad_prices))\n            if bad_prices else\n            \"Price formats look good.\"\n        )\n        return EvaluatorLog(\n            evaluator_id=self.uid,\n            name=self.name,\n            outputs={\"score\": score, \"feedback\": feedback},\n        )\n</code></pre> Step 5 - Orchestrate: simulate \u2192 evaluate \u2192 optimize <pre><code>import asyncio\n\nfrom relai import AsyncRELAI, AsyncSimulator, random_env_generator\nfrom relai.critico import Critico\nfrom relai.maestro import Maestro, params\nfrom relai.mocker import Persona, MockTool  # (already imported in Step 1 if single file)\n\nasync def main() -&gt; None:\n    # 5.1 \u2014 Set up your simulation environment\n    env_generator = random_env_generator(\n        config_set={\n            \"__main__.get_user_query\": [Persona(user_persona=\"A polite and curious user.\")],\n            \"__main__.retriever\": [MockTool(model=MODEL)],\n        }\n    )\n\n    async with AsyncRELAI() as client:\n        # 5.2 \u2014 SIMULATE\n        simulator = AsyncSimulator(agent_fn=agent_fn, env_generator=env_generator, client=client)\n        agent_logs = await simulator.run(num_runs=1)\n\n        # 5.3 \u2014 EVALUATE\n        critico = Critico(client=client)\n        critico.add_evaluators({PriceFormatEvaluator(): 1.0})\n        critico_logs = await critico.evaluate(agent_logs)\n\n        # Publish evaluation report to the RELAI platform\n        await critico.report(critico_logs)\n\n        # 5.4 \u2014 OPTIMIZE with Maestro\n        maestro = Maestro(client=client, agent_fn=agent_fn, log_to_platform=True, name=AGENT_NAME)\n        maestro.add_setup(simulator=simulator, critico=critico)\n\n        # 5.4.1 \u2014 Optimize agent configurations\n        # params.load(\"saved_config.json\")  # load previous params if available\n        await maestro.optimize_config(\n            total_rollouts=50,\n            batch_size=2,\n            explore_radius=5,\n            explore_factor=0.5,\n            verbose=True,\n        )\n        params.save(\"saved_config.json\")  # save optimized params for future usage\n\n        # 5.4.2 \u2014 Optimize agent structure\n        await maestro.optimize_structure(\n            total_rollouts=10,\n            code_paths=[\"agentic-rag.py\"],\n            verbose=True,\n        )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"maestro.html","title":"Maestro","text":""},{"location":"maestro.html#relai.maestro.Maestro","title":"<code>relai.maestro.Maestro(client, agent_fn, goal=None, max_memory=20, max_proposals=3, name='No Name', log_to_platform=True)</code>","text":"<p>Maestro automatically optimizes an AI agent to maximize its Critico score, navigating the space of configurations to intelligently improve performance on the chosen criteria.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncRELAI</code> <p>An instance of the AsyncRELAI client to interact with the RELAI platform.</p> required <code>agent_fn</code> <code>AsyncAgent</code> <p>The agent function to be optimized.</p> required <code>goal</code> <code>str</code> <p>Optional description of the goal of optimization. If None, increasing evaluation score will be considered as the only goal. Defaults to None.</p> <code>None</code> <code>max_memory</code> <code>int</code> <p>Control the maximum number of previous optimization history visible at each optimization step. Defaults to 20.</p> <code>20</code> <code>max_proposals</code> <code>int</code> <p>Control the maximum number of proposals to consider at each optimization step. Defaults to 3.</p> <code>3</code> <code>name</code> <code>str</code> <p>Name of the configuration optimization visualization on RELAI platform. Defaults to \"No Name\".</p> <code>'No Name'</code> <code>log_to_platform</code> <code>bool</code> <p>Whether to log optimization progress and results on RELAI platform. Defaults to True.</p> <code>True</code>"},{"location":"maestro.html#relai.maestro.Maestro.add_setup","title":"<code>add_setup(simulator, critico, weight=1)</code>","text":"<p>Add a new setup consisting of a simulator and a critico to Maestro.</p> <p>Parameters:</p> Name Type Description Default <code>simulator</code> <code>AsyncSimulator</code> <p>An AsyncSimulator to run the agent in the new setup.</p> required <code>critico</code> <code>Critico</code> <p>A Critico with evaluators for the new setup.</p> required <code>weight</code> <code>float</code> <p>A positive float representing the weight of this setup in comparson to others. Defaults to 1.</p> <code>1</code>"},{"location":"maestro.html#relai.maestro.Maestro.optimize_config","title":"<code>optimize_config(total_rollouts, batch_size=4, explore_radius=5, explore_factor=0.5, verbose=True)</code>  <code>async</code>","text":"<p>Optimize the configs (parameters) of the agent.</p> <p>Parameters:</p> Name Type Description Default <code>total_rollouts</code> <code>int</code> <p>Total number of rollouts to use for optimization.</p> required <code>batch_size</code> <code>int</code> <p>Base batch size to use for individual optimization steps. Defaults to 4.</p> <code>4</code> <code>explore_radius</code> <code>int</code> <p>A positive integer controlling the aggressiveness of exploration during optimization. A larger <code>explore_radius</code> encourages the optimizer to make more substantial changes between successive configurations. Defaults to 5.</p> <code>5</code> <code>explore_factor</code> <code>float</code> <p>A float between 0 to 1 controlling the exploration-exploitation trade-off. A higher <code>explore_factor</code> allocates more rollouts to discover new configs, while a lower value allocates more rollouts to ensure the discovered configs are thoroughly evaluated. Defaults to 0.5.</p> <code>0.5</code> <code>verbose</code> <code>bool</code> <p>If True, related information will be printed during the optimization step. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input parameters are not valid.</p>"},{"location":"maestro.html#relai.maestro.Maestro.optimize_structure","title":"<code>optimize_structure(total_rollouts, description=None, code_paths=None, name='No Name', verbose=True)</code>  <code>async</code>","text":"<p>Propose structural changes (i.e. changes that cannot be achieved by setting parameters alone) to improve the agent.</p> <p>Parameters:</p> Name Type Description Default <code>total_rollouts</code> <code>int</code> <p>Total number of rollouts to use for optimization. Generally, a moderate number of rollouts (e.g. 10-20) is required and recommended. For agents with longer execution traces: Try reducing the number of rollouts if an error is raised.</p> required <code>description</code> <code>str</code> <p>Text description of the current structure/workflow/... of the agent.</p> <code>None</code> <code>code_paths</code> <code>list[str]</code> <p>A list of paths corresponding to code files containing the implementation of the agent.</p> <code>None</code> <code>name</code> <code>str</code> <p>Name of the graph optimization visualization on RELAI platform. Defaults to \"No Name\".</p> <code>'No Name'</code> <code>verbose</code> <code>bool</code> <p>If True, related information will be printed during the optimization. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Suggestion for structural changes to the agent.</p>"},{"location":"mockers.html","title":"Mockers","text":""},{"location":"mockers.html#relai.mocker.Persona","title":"<code>relai.mocker.Persona(user_persona, intent=None, starting_message=None, tools=None, model='gpt-5-mini')</code>","text":"<p>               Bases: <code>BaseMocker</code></p> <p>A mocker class for creating AI personas with specific behaviors and tools. A persona can be used to mimic a particular role by defining a system prompt and optionally equipping it with tools.</p> <p>Attributes:</p> Name Type Description <code>user_persona</code> <code>str</code> <p>The description of the persona's characteristics and behavior.</p> <code>intent</code> <code>str | None</code> <p>The intent or goal of the persona during interactions.</p> <code>starting_message</code> <code>str | None</code> <p>An optional initial message that the persona will use to start interactions.</p> <code>tools</code> <code>Optional[list[Tool]]</code> <p>A list of tools that the persona can use.</p> <p>Initializes the Persona with a description, intent, optional starting message, tools, and model.</p> <p>Parameters:</p> Name Type Description Default <code>user_persona</code> <code>str</code> <p>The description of the persona's characteristics and behavior.</p> required <code>intent</code> <code>str | None</code> <p>The intent or goal of the persona during interactions.</p> <code>None</code> <code>starting_message</code> <code>str | None</code> <p>An optional initial message that the persona will use to start interactions.</p> <code>None</code> <code>tools</code> <code>Optional[list[Tool]]</code> <p>A list of tools that the persona can use.</p> <code>None</code> <code>model</code> <code>str | None</code> <p>The AI model to use for simulating the persona's behavior.</p> <code>'gpt-5-mini'</code>"},{"location":"mockers.html#relai.mocker.MockTool","title":"<code>relai.mocker.MockTool(model='gpt-5-mini')</code>","text":"<p>               Bases: <code>BaseMocker</code></p> <p>A mocker class for simulating the behavior of a tool used by an AI agent.</p> <p>Initializes the MockTool with an optional model specification.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | None</code> <p>The AI model to use for simulating the tool's behavior.</p> <code>'gpt-5-mini'</code>"},{"location":"simulator.html","title":"Simulator","text":""},{"location":"simulator.html#relai.simulator.simulated","title":"<code>relai.simulator.simulated(func)</code>","text":"<p>Decorator to mark a function to be simulated using a mocker in simulation mode. All such functions must have a corresponding mocker set in the simulation configuration. Supports both synchronous and asynchronous functions.</p>"},{"location":"simulator.html#relai.simulator.EnvGenerator","title":"<code>relai.simulator.EnvGenerator</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for functions that generate simulation configurations. These functions take an optional RELAISample and return a SimulationConfigT which is a dictionary mapping qualified function names of functions decorated with <code>@simulated</code> to their respective mocker instances.</p>"},{"location":"simulator.html#relai.simulator.random_env_generator","title":"<code>relai.simulator.random_env_generator(config_set)</code>","text":"<p>An environment generator that uniformly samples a mocker for each simulated function from the provided set of mockers.</p> <p>Parameters:</p> Name Type Description Default <code>config_set</code> <code>dict[str, Sequence[BaseMocker]]</code> <p>A mapping from qualified function names to a sequence of possible mockers for that function.</p> required"},{"location":"simulator.html#relai.simulator.SyncSimulator","title":"<code>relai.simulator.SyncSimulator(agent_fn, env_generator=None, benchmark=None, log_runs=True, client=None)</code>","text":"<p>               Bases: <code>BaseSimulator</code></p> <p>A simulator for synchronous agent functions.</p> <p>Parameters:</p> Name Type Description Default <code>agent_fn</code> <code>SyncAgent</code> <p>The synchronous agent function to be simulated.</p> required <code>env_generator</code> <code>EnvGenerator | None</code> <p>An optional environment generator function. If not provided, a default generator that returns an empty configuration will be used. The default generator can only be used with simulations that don't require any mockers.</p> <code>None</code> <code>benchmark</code> <code>Benchmark | None</code> <p>An optional benchmark to source simulation samples from.</p> <code>None</code> <code>log_runs</code> <code>bool</code> <p>Whether to log the runs to the RELAI platform. Defaults to True.</p> <code>True</code> <code>client</code> <code>RELAI | None</code> <p>A synchronous RELAI client for logging. Must be provided if log_runs is True.</p> <code>None</code>"},{"location":"simulator.html#relai.simulator.SyncSimulator.rerun","title":"<code>rerun(simulation_tapes)</code>","text":"<p>Rerun the simulator for a list of simulation tapes.</p> <p>Parameters:</p> Name Type Description Default <code>simulation_tapes</code> <code>list[SimulationTape]</code> <p>The list of simulation tapes to rerun. This allows for re-executing the agent in an environment identical to a previous run and is useful for debugging and optimization.</p> required"},{"location":"simulator.html#relai.simulator.SyncSimulator.run","title":"<code>run(num_runs)</code>","text":"<p>Run the simulator for a specified number of times.</p> <p>Parameters:</p> Name Type Description Default <code>num_runs</code> <code>int</code> <p>The number of simulation runs to execute.</p> required"},{"location":"simulator.html#relai.simulator.AsyncSimulator","title":"<code>relai.simulator.AsyncSimulator(agent_fn, env_generator=None, benchmark=None, log_runs=True, client=None)</code>","text":"<p>               Bases: <code>BaseSimulator</code></p> <p>A simulator for asynchronous agent functions.</p> <p>Parameters:</p> Name Type Description Default <code>agent_fn</code> <code>AsyncAgent</code> <p>The asynchronous agent function to be simulated.</p> required <code>env_generator</code> <code>EnvGenerator | None</code> <p>An optional environment generator function. If not provided, a default generator that returns an empty configuration will be used. The default generator can only be used with simulations that don't require any mockers.</p> <code>None</code> <code>benchmark</code> <code>Benchmark | None</code> <p>An optional benchmark to source simulation samples from.</p> <code>None</code> <code>log_runs</code> <code>bool</code> <p>Whether to log the runs to the RELAI platform. Defaults to True.</p> <code>True</code> <code>client</code> <code>RELAI | None</code> <p>A asynchronous RELAI client for logging. Must be provided if log_runs is True.</p> <code>None</code>"},{"location":"simulator.html#relai.simulator.AsyncSimulator.rerun","title":"<code>rerun(simulation_tapes)</code>  <code>async</code>","text":"<p>Rerun the simulator for a list of simulation tapes.</p> <p>Parameters:</p> Name Type Description Default <code>simulation_tapes</code> <code>list[SimulationTape]</code> <p>The list of simulation tapes to rerun. This allows for re-executing the agent in an environment identical to a previous run and is useful for debugging and optimization.</p> required"},{"location":"simulator.html#relai.simulator.AsyncSimulator.run","title":"<code>run(num_runs)</code>  <code>async</code>","text":"<p>Run the simulator for a specified number of times.</p> <p>Parameters:</p> Name Type Description Default <code>num_runs</code> <code>int</code> <p>The number of simulation runs to execute.</p> required"},{"location":"types.html","title":"Types","text":""},{"location":"types.html#relai.data.RELAISample","title":"<code>relai.data.RELAISample(benchmark_id='default', id=(lambda: uuid4().hex)(), split='All', agent_inputs=dict(), extras=dict(), serialized_simulation_config=dict())</code>  <code>dataclass</code>","text":"<p>Represents a single sample in a RELAI benchmark.</p> <p>Attributes:</p> Name Type Description <code>benchmark_id</code> <code>str</code> <p>The identifier of the benchmark this sample belongs to.</p> <code>id</code> <code>str</code> <p>The unique identifier for this sample.</p> <code>split</code> <code>str</code> <p>The data split this sample belongs to (e.g., \"Train\", \"Validation\", \"Test\").</p> <code>agent_inputs</code> <code>AgentInputs</code> <p>The inputs provided to the agent from this sample.</p> <code>extras</code> <code>Extras</code> <p>Any additional metadata or information associated with this sample. Use this field to also store evaluator-specific inputs.</p> <code>serialized_simulation_config</code> <code>dict[str, Any]</code> <p>The serialized simulation configuration for this sample. Can be used to reconstruct any mockers used in a previous simulation.</p>"},{"location":"types.html#relai.data.SimulationTape","title":"<code>relai.data.SimulationTape(sample=None, id=(lambda: uuid4().hex)(), simulation_config=dict())</code>  <code>dataclass</code>","text":"<p>A simulation tape records any inputs, outputs, and any other relevant data during a simulation.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The unique identifier for this simulation tape.</p> <code>benchmark_id</code> <code>str</code> <p>The identifier of the benchmark from which the sample used to initialize this tape was taken. If no sample was provided, defaults to \"default\".</p> <code>sample_id</code> <code>str</code> <p>The identifier of the sample used to initialize this tape. If no sample was provided, defaults to the tape's id.</p> <code>split</code> <code>str</code> <p>The data split of the sample used to initialize this tape. If no sample was provided, defaults to \"All\".</p> <code>agent_inputs</code> <code>AgentInputs</code> <p>The inputs provided to the agent from the sample used to initialize this tape. If no sample was provided, defaults to an empty dictionary.</p> <code>extras</code> <code>Extras</code> <p>Any additional metadata or information associated with this tape. If no sample was provided, defaults to an empty dictionary.</p> <code>evaluator_group</code> <code>str</code> <p>The evaluator group associated with this tape, typically set to the benchmark_id of the sample. If no sample was provided, defaults to \"default\". Can be modified in the <code>agent_fn</code>.</p> <code>simulation_config</code> <code>SimulationConfigT</code> <p>The simulation configuration used during this simulation which is a mapping from qualified function names to their respective mocker instances (Persona, MockTool etc.,).</p>"},{"location":"types.html#relai.data.AgentLog","title":"<code>relai.data.AgentLog(simulation_tape, agent_outputs=dict(), trace_id=None)</code>  <code>dataclass</code>","text":"<p>Log of a single agent simulation run.</p> <p>Attributes:</p> Name Type Description <code>simulation_tape</code> <code>SimulationTape</code> <p>The simulation tape containing inputs and metadata.</p> <code>agent_outputs</code> <code>AgentOutputs</code> <p>The outputs generated by the agent during the simulation.</p> <code>trace_id</code> <code>str | None</code> <p>An optional trace identifier for the simulation run.</p>"},{"location":"types.html#relai.data.EvaluatorLog","title":"<code>relai.data.EvaluatorLog(evaluator_id, name, outputs, config=dict())</code>  <code>dataclass</code>","text":"<p>Log of a single evaluator run.</p> <p>Attributes:</p> Name Type Description <code>evaluator_id</code> <code>str</code> <p>The ID of the evaluator.</p> <code>name</code> <code>str</code> <p>The name of the evaluator.</p> <code>outputs</code> <code>EvaluatorOutputs</code> <p>The outputs generated by the evaluator.</p> <code>config</code> <code>dict[str, Any]</code> <p>The configuration settings used for the evaluator.</p>"},{"location":"types.html#relai.data.CriticoLog","title":"<code>relai.data.CriticoLog(agent_log, evaluator_logs=list(), aggregate_score=0.0, aggregate_feedback='', trace_id=None)</code>  <code>dataclass</code>","text":"<p>Log of a Critico evaluation run.</p> <p>Attributes:</p> Name Type Description <code>agent_log</code> <code>AgentLog</code> <p>The log of the agent simulation run.</p> <code>evaluator_logs</code> <code>list[EvaluatorLog]</code> <p>A list of logs from individual evaluators.</p> <code>aggregate_score</code> <code>float</code> <p>The aggregate score computed from all the evaluator logs.</p> <code>aggregate_feedback</code> <code>str</code> <p>The aggregate feedback compiled from all the evaluator logs.</p> <code>trace_id</code> <code>str | None</code> <p>An optional trace identifier for the corresponding agent simulation run.</p>"},{"location":"overview/basic-usage.html","title":"Basic usage","text":"Accessing RELAI Benchmarks <p>RELAI benchmarks comprise pre-defined collections of <code>Sample</code> objects, serving as standardized test sets for evaluating AI agent capabilities. Each benchmark is assigned a unique identifier which can be obtained from the metadata of the benchmark on the RELAI platform. We currently support the following benchmarks:</p> RELAI Benchmark Agent Input Fields Evaluator Input Fields RELAIQuestionAnsweringBenchmark <code>question</code> <code>rubrics</code>, <code>std_answer</code> RELAISummarizationBenchmark <code>source</code> <code>key_facts</code>, <code>style_rubrics</code>, <code>format_rubrics</code> <p>To load a benchmark, instantiate the corresponding <code>RELAIBenchmark</code> class using the benchmark ID (You can find the benchmark ID in the 'Additional Details' section of a benchmark). This action initiates a connection to the RELAI platform and retrieves all associated samples. For example, to create a <code>RELAIQuestionAnsweringBenchmark</code>:</p> <pre><code>from relai.critico.benchmark import RELAIQuestionAnsweringBenchmark\n\n# &lt;benchmark_id&gt; is available under \"Additional Details\".\n# Make sure that &lt;benchmark_id&gt; corresponds to a question-answering benchmark. \nbenchmark = RELAIQuestionAnsweringBenchmark(\"&lt;benchmark_id&gt;\")\n\n# All RELAIBenchmark instances are iterable. Inspection of a sample:\nfor i, sample in enumerate(benchmark):\n    print(f\"Sample ID: {sample.sample_id}\")\n    print(f\"Agent Inputs (for AI agent processing): {sample.agent_inputs}\")\n    print(f\"Evaluator Inputs (for evaluating AI agent outputs): {sample.eval_inputs}\")\n    break # Displaying only the first sample for brevity\n</code></pre> <p>A <code>RELAIBenchmark</code> object functions as an iterable collection of <code>Sample</code> objects. Each <code>Sample</code> is a Pydantic model that encapsulates the requisite data for both agent processing and subsequent evaluation.</p> AI Agent Response Generation <p><code>Sample.agent_inputs</code> contains all the inputs need by an agent. </p> <pre><code>def custom_ai_agent(source: str **kwargs) -&gt; dict:\n    \"\"\"\n    A placeholder AI agent\n    \"\"\"\n    return {\"summary\": f\"A concise summary of the provided text: '{source[:50]}...'\"}\n\nraw_agent_output = agent(**sample.agent_inputs)  # `agent_inputs` must contain the key `source`\n</code></pre> <p>To evaluate the response subsequently using an <code>Evaluator</code>, encapsulate the raw agent output in <code>AgentResponse</code>.</p> <pre><code>from relai.critico.benchmark import AgentResponse\n\nagent_response = AgentResponse(\n    sample=sample,\n    agent_outputs=raw_agent_output\n)\n</code></pre> AI Agent Evaluation <p>We provide a diverse set of evaluators for automated quality assessment. Each evaluator is specialized for a distinct evaluation criterion (e.g., length, content fidelity, stylistic adherence). Many evaluators support configurable hyperparameters to fine-tune their assessment behavior. For instance, <code>RELAILengthEvaluator</code> permits specification of the measurement unit (sentences, words, characters) and an acceptable_range. To evaluate an agent response, first instantiate the evaluator:</p> <pre><code>from relai.critico.evaluate import RELAILengthEvaluator\n\nlength_evaluator = RELAILengthEvaluator(\n    measure=\"sentences\",\n    acceptable_range=(10, 15),\n)\n</code></pre> <p>Evaluator instances are callable objects. An <code>AgentResponse</code> object can be directly passed to an evaluator instance to initiate the assessment. The evaluator will then execute its defined logic and return an <code>EvaluatorResponse</code>. Each evaluator expects a list of input fields. Each of these fields must be present in either <code>agent_inputs</code> (of the sample), <code>eval_inputs</code> (of the sample), or <code>agent_outputs</code> (of the agent response).</p> <pre><code>evaluator_response = length_evaluator(agent_response)\nprint(f\"Score: {evaluator_response.score}\")\nprint(f\"Feedback: {evaluator_response.feedback}\")\n</code></pre> <p>We currently support the following evaluators:</p> RELAI Evaluator Required Fields RELAILengthEvaluator <code>source</code>, <code>summary</code> RELAIContentEvaluator <code>key_facts</code>, <code>summary</code> RELAIHallucinationEvaluator <code>source</code>, <code>summary</code> RELAIStyleEvaluator <code>style_rubrics</code>, <code>summary</code> RELAIFormatEvaluator <code>format_rubrics</code>, <code>summary</code> RELAIRubricBasedEvaluator <code>question</code>, <code>answer</code>, <code>rubrics</code>, <code>std_answer</code> <p>You can also define custom evaluators by directly inheriting from the Evaluator class and overriding the <code>compute_evaluator_result</code> method. Example:</p> <pre><code>from typing import Optional, Callable\nfrom relai.critico.benchmark import AgentResponse\nfrom relai.critico.evaluate import Evaluator, EvaluatorResponse\n\nclass CustomMCQEvaluator(Evaluator):\n    \"\"\"\n    An evaluator for multiple choice questions. For a correct answer, assigns a score equal to the number of\n    options in the question (presumably an indicator of difficulty). Assigns 0 for an incorrect answer.\n    \"\"\"\n    def __init__(self, transform: Optional[Callable] = None, num_options: int = 4)\n        super().__init__(\n            name=\"custom-evaluator\",\n            required_fields=[\"chosen_answer\", \"correct_answer\"],  # Evaluator expects a `chosen_answer` field in `agent_response` and a `correct_answer` in `eval_inputs`\n            transform=transform\n            num_options=4\n        )\n\n    def compute_evaluator_result(self, agent_response: AgentResponse) -&gt; EvaluatorResponse:\n        chosen_answer = agent_response.agent_outputs[\"chosen_answer\"]\n        correct_answer = agent_response.sample.eval_inputs[\"eval_inputs\"]\n        if chosen_answer == correct_answer:\n            score = self.hyperparameters[\"num_options\"]\n            feedback = \"Answer is correct\"\n        else:\n            score = 0\n            feedback = f\"Answer is incorrect. The correct answer is {correct_answer}\"\n        return EvaluatorResponse(\n            evaluator_id=self.uid,\n            evaluator_name=self.name,\n            evaluator_configuration=self.hyperparameters,\n            agent_response=agent_response,\n            score=score,\n            feedback=feedback\n        )\n</code></pre>"},{"location":"overview/using-critico.html","title":"Using critico","text":"<p>Critico allows for a comprehensive evaluation of your AI agents using multiple benchmarks and evaluators. To set up Critico, first follow the instructions in Basic Usage to set up the benchmark and any evaluators you want to use.</p> <pre><code>from relai.critico.benchmark import RELAIQuestionAnsweringBenchmark\nfrom relai.critico.evaluate import RELAILengthEvaluator\n\nbenchmark = RELAIQuestionAnsweringBenchmark(\"&lt;benchmark_id&gt;\")\n\nlength_evaluator = RELAILengthEvaluator(\n    measure=\"sentences\",\n    acceptable_range=(10, 15),\n)\n</code></pre> Evaluate using Critico <p>Evaluating using Critico involves adding ore or more Evaluators using the <code>add_benchmark</code> method as follows:</p> <pre><code>from relai.critico import Critico\n\ncritico = Critico(agent_name=\"summarizer:v1.0\")\ncritico.add_benchmark(\n    benchmark=benchmark,\n    evaluators={length_evaluator: 1.0},  # 1.0 is the weight for the score computed by the evaluator\n)\ncritico_feedbacks = critico.evaluate(responses=[agent_response])\nprint(critico_feedbacks[0].aggregate_score, critico_feedbacks[0].aggregate_feedback)\n</code></pre> Viewing Evaluation Results on the RELAI Platform <p>To view the evaluation results generated by Critico on the platform, use the <code>report_evaluation</code> method:</p> <pre><code>from relai.critico import Critico\n\ncritico = Critico(agent_name=\"summarizer:v1.0\")\ncritico.add_benchmark(\n    benchmark=benchmark,\n    evaluators={length_evaluator: 1.0},  # 1.0 is the weight for the score computed by the evaluator\n)\ncritico_feedbacks = critico.evaluate(responses=[agent_response])\ncritico.report_evaluation(title=\"Evaluation\", feedbacks=critico_feedbacks)\n</code></pre>"},{"location":"tutorials/custom-evaluation.html","title":"Custom evaluation","text":"<p>The following script is a complete example showing how to evaluate an AI agent using a custom benchmark and evaluator:</p> <pre><code>import asyncio\nimport os\nfrom typing import Callable, Optional\n\nfrom relai import AsyncRELAI\n\n# Import core components for evaluation\nfrom relai.critico import Critico\nfrom relai.critico.benchmark import AgentResponse, CSVBenchmark\nfrom relai.critico.evaluate import Evaluator, EvaluatorResponse\n\n\nasync def mock_sentiment_agent(text: str) -&gt; str:\n    \"\"\"\n    Mock function to simulate a sentiment analysis agent.\n    In a real scenario, this would call your actual sentiment analysis model.\n\n    Args:\n        text: The input text to analyze for sentiment\n\n    Returns:\n        A sentiment label (positive, negative, or neutral)\n    \"\"\"\n    # Simple mock logic based on keywords\n    positive_words = [\"good\", \"great\", \"excellent\", \"amazing\", \"wonderful\", \"love\"]\n    negative_words = [\"bad\", \"terrible\", \"awful\", \"hate\", \"horrible\", \"disappointing\"]\n\n    text_lower = text.lower()\n    positive_count = sum(1 for word in positive_words if word in text_lower)\n    negative_count = sum(1 for word in negative_words if word in text_lower)\n\n    if positive_count &gt; negative_count:\n        return \"positive\"\n    elif negative_count &gt; positive_count:\n        return \"negative\"\n    else:\n        return \"neutral\"\n\n\nclass CustomSentimentEvaluator(Evaluator):\n    \"\"\"\n    A custom evaluator for sentiment analysis tasks.\n\n    This evaluator compares the agent's predicted sentiment against the ground truth\n    and provides detailed scoring based on prediction accuracy and confidence.\n    \"\"\"\n\n    def __init__(\n        self,\n        transform: Optional[Callable] = None,\n        correct_score: float = 1.0,\n        incorrect_score: float = 0.0,\n        partial_credit: bool = True,\n    ):\n        \"\"\"\n        Initialize the custom sentiment evaluator.\n\n        Args:\n            transform: Optional function to transform agent outputs\n            correct_score: Score to assign for correct predictions\n            incorrect_score: Score to assign for incorrect predictions\n            partial_credit: Whether to give partial credit for neutral predictions\n        \"\"\"\n        super().__init__(\n            name=\"custom-sentiment-evaluator\",\n            # Specify required fields from the benchmark and agent response\n            required_fields=[\"text\", \"predicted_sentiment\", \"true_sentiment\"],\n            transform=transform,\n            # Store configuration as hyperparameters\n            correct_score=correct_score,\n            incorrect_score=incorrect_score,\n            partial_credit=partial_credit,\n        )\n\n    async def compute_evaluator_result(self, agent_response: AgentResponse) -&gt; EvaluatorResponse:\n        \"\"\"\n        Evaluate the agent's sentiment prediction against ground truth.\n\n        Args:\n            agent_response: Contains the sample and agent's outputs\n\n        Returns:\n            EvaluatorResponse with score and feedback\n        \"\"\"\n        # Extract required fields from different sources\n        text = agent_response.sample.agent_inputs[\"text\"]\n        predicted_sentiment = agent_response.agent_outputs[\"predicted_sentiment\"]\n        true_sentiment = agent_response.sample.eval_inputs[\"true_sentiment\"]\n\n        # Evaluate prediction accuracy\n        if predicted_sentiment.lower() == true_sentiment.lower():\n            score = self.hyperparameters[\"correct_score\"]\n            feedback = f\"Correct! Predicted '{predicted_sentiment}' matches true sentiment '{true_sentiment}'\"\n        elif (\n            self.hyperparameters[\"partial_credit\"]\n            and predicted_sentiment.lower() == \"neutral\"\n            and true_sentiment.lower() in [\"positive\", \"negative\"]\n        ):\n            # Give partial credit for neutral predictions on polar sentiments\n            score = self.hyperparameters[\"correct_score\"] * 0.5\n            feedback = f\"Partial credit: Predicted neutral for {true_sentiment} sentiment\"\n        else:\n            score = self.hyperparameters[\"incorrect_score\"]\n            feedback = f\"Incorrect: Predicted '{predicted_sentiment}' but true sentiment is '{true_sentiment}'\"\n\n        # Add text length as additional context\n        text_length = len(text.split())\n        feedback += f\" (Text length: {text_length} words)\"\n\n        return EvaluatorResponse(\n            evaluator_id=self.uid,\n            evaluator_name=self.name,\n            evaluator_configuration=self.hyperparameters,\n            agent_response=agent_response,\n            score=score,\n            feedback=feedback,\n        )\n\n\nasync def main():\n    # Set up authentication with RELAI platform (required for result reporting)\n    os.environ[\"RELAI_API_KEY\"] = (\n        \"relai-...\"  # API key from https://platform.relai.ai/settings/access/api-keys\n    )\n\n    # Create a custom benchmark from a CSV file\n    # The CSV file should have columns: text, true_sentiment, confidence_level, domain\n    # Example CSV content:\n    # text,true_sentiment,confidence_level,domain\n    # \"This movie is absolutely amazing!\",positive,high,entertainment\n    # \"The service was disappointing\",negative,medium,service\n    # \"It's an okay product I guess\",neutral,low,product\n    benchmark = CSVBenchmark(\n        csv_file=\"sentiment_benchmark.csv\",  # Path to your CSV file\n        # Specify which columns are inputs for the agent\n        agent_input_columns=[\"text\"],\n        # Specify which columns are inputs for evaluators\n        eval_input_columns=[\"true_sentiment\", \"confidence_level\", \"domain\"],\n        benchmark_id=\"custom-sentiment-benchmark\",\n    )\n\n    # Initialize list to store agent responses\n    responses = []\n\n    # Generate agent responses for each sample in the benchmark\n    print(f\"Processing {len(benchmark)} samples from CSV benchmark...\")\n    for sample in benchmark:\n        # Create agent response with the required output format\n        agent_response = AgentResponse(\n            sample=sample,\n            # Agent outputs must match what evaluators expect\n            agent_outputs={\"predicted_sentiment\": await mock_sentiment_agent(sample.agent_inputs[\"text\"])},\n        )\n        responses.append(agent_response)\n\n    # Create instances of custom evaluators with different configurations\n    # Primary evaluator focuses on accuracy\n    sentiment_evaluator = CustomSentimentEvaluator(\n        correct_score=1.0,\n        incorrect_score=0.0,\n        partial_credit=True,  # Give partial credit for neutral predictions\n    )\n\n    async with AsyncRELAI() as client:\n        # Create Critico instance for comprehensive evaluation\n        critico = Critico(client=client, agent_name=\"custom-sentiment-agent\")\n\n        # Register the benchmark with weighted evaluators\n        # Higher weight for accuracy evaluator since correctness is most important\n        critico.add_benchmark(\n            benchmark,\n            evaluators={\n                sentiment_evaluator: 1.0,\n            },\n        )\n\n        # Execute evaluation across all responses\n        print(\"Running evaluation with custom evaluators...\")\n        critico_feedbacks = await critico.evaluate(responses)\n\n        # Display detailed results\n        print(\"\\n=== Evaluation Results ===\")\n        for i, feedback in enumerate(critico_feedbacks):\n            sample_text = feedback.agent_response.sample.agent_inputs[\"text\"][:50] + \"...\"\n            print(f\"\\nSample {i + 1}: {sample_text}\")\n            print(f\"Aggregate Score: {feedback.aggregate_score:.3f}\")\n            print(f\"Aggregate Feedback: {feedback.aggregate_feedback}\")\n\n        # Upload results to RELAI platform for persistent analysis\n        # This creates a dashboard view with detailed breakdowns\n        await critico.report_evaluation(\"Custom Sentiment Analysis Evaluation\", critico_feedbacks)\n\n        print(\"\\nEvaluation complete! Results uploaded to RELAI platform.\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"}]}