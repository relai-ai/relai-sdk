Critico allows for a comprehensive evaluation of your AI agents using multiple benchmarks and evaluators. To set up Critico, first follow the instructions in [Basic Usage](basic-usage.md) to set up the benchmark and any evaluators you want to use.

```python
from relai.critico.benchmark import RELAIQuestionAnsweringBenchmark
from relai.critico.evaluate import RELAILengthEvaluator

benchmark = RELAIQuestionAnsweringBenchmark("<benchmark_id>")

length_evaluator = RELAILengthEvaluator(
    measure="sentences",
    acceptable_range=(10, 15),
)
```

<h2>Evaluate using Critico</h2>

Evaluating using Critico involves adding ore or more Evaluators using the [`add_benchmark`](../critico.md#relai.critico.critico.Critico.add_benchmark) method as follows:

```python
from relai.critico import Critico

critico = Critico(agent_name="summarizer:v1.0")
critico.add_benchmark(
    benchmark=benchmark,
    evaluators={length_evaluator: 1.0},  # 1.0 is the weight for the score computed by the evaluator
)
critico_feedbacks = critico.evaluate(responses=[agent_response])
print(critico_feedbacks[0].aggregate_score, critico_feedbacks[0].aggregate_feedback)
```

<h2>Viewing Evaluation Results on the RELAI Platform</h2>
To view the evaluation results generated by Critico on the platform, use the [`report_evaluation`](../critico.md#relai.critico.critico.Critico.report_evaluation) method:

```python
from relai.critico import Critico

critico = Critico(agent_name="summarizer:v1.0")
critico.add_benchmark(
    benchmark=benchmark,
    evaluators={length_evaluator: 1.0},  # 1.0 is the weight for the score computed by the evaluator
)
critico_feedbacks = critico.evaluate(responses=[agent_response])
critico.report_evaluation(title="Evaluation", feedbacks=critico_feedbacks)
```