{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Prereqs:\n",
    "#   export RELAI_API_KEY=\"relai-...\"        # your RELAI API key\n",
    "#   export OPENAI_API_KEY=\"sk-...\"          # if your agent/tool uses OpenAI\n",
    "#   pip install relai                   # relai\n",
    "#   pip install openinference-instrumentation-openai-agents  # optional automatic tracing for openai agents SDK\n",
    "#\n",
    "# Here we demonstrate with a simple summarization agent:\n",
    "# 1. How to run agents in a simulated environment and collect simulation traces/runs.\n",
    "# 2. How to annotate the simulation runs on RELAI platform (platform.relai.ai) and create an Annotation Benchmark\n",
    "# 3. (next on `summarization-agent (simulate->annotate->optimize)-part-2.py`) How to optimize the agent over an annotation benchmark.\n",
    "\n",
    "!pip install relai\n",
    "!pip install openinference-instrumentation-openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b14433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"RELAI_API_KEY\"] = \"relai-...\"  # or set permanently in your system\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # or set permanently in your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor\n",
    "\n",
    "from relai import AsyncRELAI, simulated\n",
    "from relai.benchmark import RELAIAnnotationBenchmark\n",
    "from relai.critico import Critico\n",
    "from relai.critico.evaluate import RELAIAnnotationEvaluator\n",
    "from relai.data import RELAISample, SimulationTape\n",
    "from relai.logger import tracer_provider\n",
    "from relai.maestro import Maestro, params, register_param\n",
    "from relai.mocker.persona import Persona\n",
    "from relai.simulator import AsyncSimulator, random_env_generator\n",
    "\n",
    "# ---- Observability (optional but recommended) -------------------------------\n",
    "OpenAIAgentsInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1 — Decorate inputs/tools that will be simulated\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "@simulated\n",
    "async def get_user_input():\n",
    "    msg = input(\"User: \")\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a584dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2 — Your agent core\n",
    "# (additional) To optimize in STEP 5.4, use `register_param` to define tunable\n",
    "# parameters and `params` to access them in your agent.\n",
    "# ============================================================================\n",
    "\n",
    "register_param(\n",
    "    \"model\",\n",
    "    type=\"model\",\n",
    "    init_value=\"gpt-4.1-mini\",\n",
    "    desc=\"LLM model for the agent\",\n",
    "    allowed=[\"gpt-4o-mini\", \"gpt-4.1-mini\", \"gpt-5-mini\"],\n",
    ")\n",
    "register_param(\"prompt\", type=\"prompt\", init_value=\"Summarize the given text.\", desc=\"system prompt for the agent\")\n",
    "\n",
    "\n",
    "async def summarization_agent(msg: str):\n",
    "    agent = Agent(name=\"Summarization Agent\", instructions=params.prompt, model=params.model)\n",
    "    response = await Runner.run(agent, msg)\n",
    "    return response.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698fed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3 — Wrap agent for simulation traces\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "async def agent_fn(tape: SimulationTape):\n",
    "    input = await get_user_input()\n",
    "    print(\"User:\", input)  # Debug print\n",
    "    tape.agent_inputs[\"user_text\"] = input  # trace inputs for later auditing\n",
    "    response = await summarization_agent(input)\n",
    "    return {\"response\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4 — Simulate\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "async def simulate():\n",
    "    # 4.1 — Set up your simulation environment\n",
    "    # Bind Personas/MockTools to fully-qualified function names\n",
    "    env_generator = random_env_generator(\n",
    "        {\n",
    "            \"__main__.get_user_input\": [\n",
    "                Persona(user_persona=\"You have a piece of news to summarize. Include that as part of your message.\"),\n",
    "                Persona(\n",
    "                    user_persona=\"You have a piece of article to summarize. Include that as part of your message.\"\n",
    "                ),\n",
    "            ]\n",
    "        }\n",
    "        # Alternatively, set up a Persona Set through RELAI platform (platform.relai.ai) and use the code below:\n",
    "        # {\"__main__.get_user_input\": PersonaSet(persona_set_id=\"your_persona_set_id_here\")}\n",
    "    )\n",
    "\n",
    "    # 4.2 — SIMULATE\n",
    "    async with AsyncRELAI() as client:\n",
    "        simulator = AsyncSimulator(\n",
    "            client=client,\n",
    "            agent_fn=agent_fn,\n",
    "            env_generator=env_generator,\n",
    "            log_runs=True,\n",
    "        )\n",
    "\n",
    "        agent_logs = await simulator.run(num_runs=4)\n",
    "        print(agent_logs)\n",
    "\n",
    "\n",
    "await simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5 — Annotate\n",
    "# ============================================================================\n",
    "\n",
    "# Go to RELAI platform (platform.relai.ai) under ->Results->Runs,\n",
    "# click on individual runs to:\n",
    "# 1. view and provide feedback to the simulation runs you just executed.\n",
    "# 2. create an Annotation Benchmark from these runs for future optimization\n",
    "#    with the \"Add to Benchmark\" button at the bottom. (IMPORTANT: Make sure\n",
    "#    only runs corresponding to the current task are included in the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6 — Simulate->Evalaute->Optimize with annotation benchmark\n",
    "# ============================================================================\n",
    "# 6.1 — Load your annotation benchmark created in STEP 5\n",
    "benchmark = RELAIAnnotationBenchmark(\n",
    "    benchmark_id=\"benchmark ID for your annotation benchmark\"\n",
    ")  # replace with your benchmark ID\n",
    "for sample in benchmark.samples:\n",
    "    print(sample)  # inspect loaded benchmark samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 — Set up a environment generator that customizes simulation configs based\n",
    "# on the benchmark samples\n",
    "def custom_env_generator(sample: RELAISample | None = None):\n",
    "    if not sample:\n",
    "        return {}\n",
    "    return {\n",
    "        \"__main__.get_user_input\": Persona(\n",
    "            user_persona=sample.extras[\"simulation_config\"][\"__main__.get_user_input\"][\"user_persona\"],\n",
    "            starting_message=sample.agent_inputs[\"all_inputs\"][\n",
    "                \"user_text\"\n",
    "            ],  # provide the original user input recorded\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook example, since the agent code is contained in the notebook,\n",
    "# we create a source.py file containing the agent code for later optimization of agent structure\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "def get_notebook_code():\n",
    "    ip = get_ipython()\n",
    "    cells = ip.user_ns[\"In\"]  # This is a list of all executed input cells as strings\n",
    "    source = \"\"\n",
    "\n",
    "    # For example, print everything except the current cell\n",
    "    for idx, code in enumerate(cells):\n",
    "        if code and not code.strip().startswith(\"get_ipython()\") and not \"import os\" in code:\n",
    "            source += code + \"\\n\"\n",
    "\n",
    "    return source\n",
    "\n",
    "\n",
    "with open(\"source.py\", \"w\") as f:\n",
    "    f.write(get_notebook_code())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    async with AsyncRELAI() as client:\n",
    "        # 6.3 — Set up the simulator with the annotation benchmark and custom env generator\n",
    "        simulator = AsyncSimulator(\n",
    "            client=client,\n",
    "            agent_fn=agent_fn,\n",
    "            env_generator=custom_env_generator,  # use custom env generator\n",
    "            benchmark=benchmark,  # use the annotation benchmark for simulation\n",
    "            log_runs=True,\n",
    "        )\n",
    "\n",
    "        # 6.4 — Set up Critico with RELAIAnnotationEvaluator for automatic evaluation of annotation benchmarks\n",
    "        critico = Critico(client=client)\n",
    "        critico.add_evaluators(evaluators={RELAIAnnotationEvaluator(client=client): 1})\n",
    "\n",
    "        # 6.5 — OPTIMIZE with Maestro\n",
    "        maestro = Maestro(client=client, agent_fn=agent_fn, log_to_platform=True, name=\"Summarization Agent\")\n",
    "        maestro.add_setup(simulator=simulator, critico=critico)\n",
    "        # one can use multiple simulator+critico setups with different weights by calling `add_setup` multiple times\n",
    "        # maestro.add_setup(simulator=simulator, critico=critico, weight = 1)\n",
    "        # maestro.add_setup(simulator=another_simulator, critico=another_critico, weight = 0.5)\n",
    "\n",
    "        # 6.5.1 — Optimize agent configurations (the parameters registered earlier in STEP 2)\n",
    "        # params.load(\"saved_config.json\")  # load previous params if available\n",
    "        await maestro.optimize_config(\n",
    "            total_rollouts=10,  # Total number of rollouts to use for optimization.\n",
    "            batch_size=2,  # Base batch size to use for individual optimization steps. Defaults to 4.\n",
    "            explore_radius=1,  # A positive integer controlling the aggressiveness of exploration during optimization.\n",
    "            explore_factor=0.5,  # A float between 0 to 1 controlling the exploration-exploitation trade-off.\n",
    "            verbose=True,  # If True, additional information will be printed during the optimization step.\n",
    "        )\n",
    "        params.save(\"saved_config.json\")  # save optimized params for future usage\n",
    "\n",
    "        # 6.5.2 — Optimize agent structure (changes that cannot be achieved by setting parameters alone)\n",
    "        await maestro.optimize_structure(\n",
    "            total_rollouts=10,  # Total number of rollouts to use for optimization.\n",
    "            code_paths=[\"source.py\"],  # A list of paths corresponding to code implementations of the agent.\n",
    "            verbose=True,  # If True, additional information will be printed during the optimization step.\n",
    "        )\n",
    "\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
