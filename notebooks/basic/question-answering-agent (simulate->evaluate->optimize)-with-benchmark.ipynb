{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87954eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Prereqs:\n",
    "#   export RELAI_API_KEY=\"relai-...\"        # your RELAI API key\n",
    "#   export GEMINI_API_KEY=\"AI...\"          # if your agent/tool uses Gemini\n",
    "#   pip install relai                  # relai\n",
    "#\n",
    "# Here we demonstrate with a question answering agent:\n",
    "# 1. How to run agents in a simulated environment based on a csv benchmark and collect simulation traces/runs.\n",
    "# 2. How to evaluate the agent's performance with custom evaluators that utilize the csv benchmark.\n",
    "# 3. How to optimize the agent based on the simulation and evaluation.\n",
    "!pip install relai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc609ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"RELAI_API_KEY\"] = \"relai-...\"  # or set permanently in your system\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AI...\"  # or set permanently in your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f88372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "from relai import (\n",
    "    AgentLog,\n",
    "    AgentOutputs,\n",
    "    AsyncRELAI,\n",
    "    AsyncSimulator,\n",
    "    EvaluatorLog,\n",
    "    SimulationTape,\n",
    ")\n",
    "from relai.benchmark import CSVBenchmark\n",
    "from relai.critico import Critico\n",
    "from relai.critico.evaluate import Evaluator\n",
    "from relai.maestro import Maestro, params, register_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36d8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1 — Load your csv data into a benchmark\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "benchmark = CSVBenchmark(\n",
    "    csv_file=\"../../examples/basic/assets/sample_questions.csv\", \n",
    "    # you can find this file under https://github.com/relai-ai/relai-sdk/blob/main/examples/assets/sample_questions.csv\n",
    "    agent_input_columns=[\"Question\"],\n",
    "    extra_columns=[\"Gold answer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a236a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2 — Your agent core\n",
    "# (additional) To optimize in STEP 5.4, use `register_param` to define tunable\n",
    "# parameters and `params` to access them in your agent.\n",
    "# ============================================================================\n",
    "\n",
    "register_param(\n",
    "    \"prompt\",\n",
    "    type=\"prompt\",\n",
    "    init_value=\"{question}\",\n",
    "    desc=\"prompt template for the agent\",\n",
    ")\n",
    "\n",
    "register_param(\n",
    "    \"model\",\n",
    "    type=\"model\",\n",
    "    init_value=\"gemini-2.5-flash\",\n",
    "    desc=\"LLM model for the agent\",\n",
    "    allowed=[\"gemini-2.5-flash\"],  # add more models as needed\n",
    ")\n",
    "\n",
    "\n",
    "async def question_answering_agent(question: str) -> dict[str, str]:\n",
    "    client = genai.Client()\n",
    "    response = client.models.generate_content(\n",
    "        model=params.model,  # access registered parameter\n",
    "        contents=params.prompt.format(question=question),  # access registered parameter\n",
    "    )\n",
    "    return response.text  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3 — Wrap agent for simulation traces\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "async def agent_fn(tape: SimulationTape) -> AgentOutputs:\n",
    "    question = tape.agent_inputs[\n",
    "        \"Question\"\n",
    "    ]  # read the question from the tape, which originates from the benchmark samples\n",
    "    \n",
    "    # It is good practice to catch exceptions in agent function\n",
    "    # especially if the agent might raise errors with different configs\n",
    "    try:\n",
    "        return {\"answer\": await question_answering_agent(question)}\n",
    "    except Exception as e:\n",
    "        return {\"answer\": f\"Error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0423a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4 — Define evaluators (Critico)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class GoldAnswerEvaluator(Evaluator):\n",
    "    \"\"\"An illustrative evaluator that checks for the presence of the gold answer in the agent's output.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(name=\"GoldAnswerEvaluator\", required_fields=[\"answer\", \"Question\", \"Gold answer\"])\n",
    "\n",
    "    async def compute_evaluator_result(self, agent_log: AgentLog) -> EvaluatorLog:\n",
    "        gold_answer = str(agent_log.simulation_tape.extras[\"Gold answer\"])\n",
    "        agent_answer = agent_log.agent_outputs[\"answer\"]\n",
    "        if gold_answer in agent_answer:\n",
    "            score = 1.0\n",
    "            feedback = f\"The agent's answer contains the gold answer: {gold_answer}.\"\n",
    "        else:\n",
    "            score = 0.0\n",
    "            feedback = f\"The agent's answer does NOT contain the gold answer: {gold_answer}.\"\n",
    "        return EvaluatorLog(evaluator_id=self.uid, name=self.name, outputs={\"score\": score, \"feedback\": feedback})\n",
    "\n",
    "# (You can add built-in RELAI platform evaluators here as well.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff699943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook example, since the agent code is contained in the notebook,\n",
    "# we create a source.py file containing the agent code for later optimization of agent structure\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "def get_notebook_code():\n",
    "    ip = get_ipython()\n",
    "    cells = ip.user_ns['In']  # This is a list of all executed input cells as strings\n",
    "    source = \"\"\n",
    "\n",
    "    # For example, print everything except the current cell\n",
    "    for idx, code in enumerate(cells):\n",
    "        if code and not code.strip().startswith(\"get_ipython()\") and not \"import os\" in code:\n",
    "            source += code + \"\\n\"\n",
    "\n",
    "    return source\n",
    "\n",
    "with open(\"source.py\", \"w\") as f:\n",
    "    f.write(get_notebook_code())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5 — Orchestrate: simulate → evaluate →  optimize\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    # 5.1 — Set up your simulation environment\n",
    "    async with AsyncRELAI() as client:\n",
    "        # 5.2 — SIMULATE\n",
    "        simulator = AsyncSimulator(\n",
    "            agent_fn=agent_fn, \n",
    "            client=client, \n",
    "            benchmark=benchmark, # IMPORTANT: use the csv benchmark for simulation\n",
    "            log_runs=True\n",
    "        )\n",
    "        agent_logs = await simulator.run(num_runs=1)\n",
    "        print(agent_logs)\n",
    "\n",
    "        # 5.3 — EVALUATE\n",
    "        critico = Critico(client=client)\n",
    "        critico.add_evaluators({GoldAnswerEvaluator(): 1.0})\n",
    "        critico_logs = await critico.evaluate(agent_logs)\n",
    "\n",
    "        # Publish evaluation report to the RELAI platform\n",
    "        await critico.report(critico_logs)\n",
    "\n",
    "        # 5.4 — OPTIMIZE with Maestro\n",
    "        maestro = Maestro(client=client, agent_fn=agent_fn, log_to_platform=True, name=\"Question Answering Agent Example\")\n",
    "        maestro.add_setup(simulator=simulator, critico=critico)\n",
    "        # one can use multiple simulator+critico setups with different weights by calling `add_setup` multiple times\n",
    "        # maestro.add_setup(simulator=simulator, critico=critico, weight = 1)\n",
    "        # maestro.add_setup(simulator=another_simulator, critico=another_critico, weight = 0.5)\n",
    "\n",
    "        # 5.4.1 — Optimize agent configurations (the parameters registered earlier in STEP 2)\n",
    "        # params.load(\"saved_config.json\")  # load previous params if available\n",
    "        await maestro.optimize_config(\n",
    "            total_rollouts=20,  # Total number of rollouts to use for optimization.\n",
    "            batch_size=2,  # Base batch size to use for individual optimization steps. Defaults to 4.\n",
    "            explore_radius=1,  # A positive integer controlling the aggressiveness of exploration during optimization.\n",
    "            explore_factor=0.5,  # A float between 0 to 1 controlling the exploration-exploitation trade-off.\n",
    "            verbose=True,  # If True, additional information will be printed during the optimization step.\n",
    "        )\n",
    "        params.save(\"saved_config.json\")  # save optimized params for future usage\n",
    "\n",
    "        # 5.4.2 — Optimize agent structure (changes that cannot be achieved by setting parameters alone)\n",
    "        await maestro.optimize_structure(\n",
    "            total_rollouts=10,  # Total number of rollouts to use for optimization.\n",
    "            code_paths=[\"source.py\"],  # A list of paths corresponding to code implementations of the agent.\n",
    "            verbose=True,  # If True, additional information will be printed during the optimization step.\n",
    "        )\n",
    "\n",
    "# asyncio.run(main()) # for python\n",
    "await main() # for notebook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
