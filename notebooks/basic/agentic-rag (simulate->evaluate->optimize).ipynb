{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00878595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Prereqs:\n",
    "#   export RELAI_API_KEY=\"relai-...\"        # your RELAI API key\n",
    "#   export OPENAI_API_KEY=\"sk-...\"          # if your agent/tool uses OpenAI\n",
    "#   pip install relai                   # relai\n",
    "#   pip install openinference-instrumentation-openai-agents  # optional tracing\n",
    "#\n",
    "# Here we demonstrate with a simple agentic RAG (Retrieval-Augmented Generation) agent:\n",
    "# 1. How to run agents in a simulated environment and collect simulation traces/runs.\n",
    "# 2. How to evaluate the agent's performance with custom evaluators.\n",
    "# 3. How to optimize the agent based on the simulation and evaluation.\n",
    "\n",
    "!pip install relai\n",
    "!pip install openinference-instrumentation-openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"RELAI_API_KEY\"] = \"relai-...\"  # or set permanently in your system\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # or set permanently in your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "from agents import Agent, Runner, function_tool\n",
    "from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor\n",
    "\n",
    "from relai import (\n",
    "    AgentLog,\n",
    "    AgentOutputs,\n",
    "    AsyncRELAI,\n",
    "    AsyncSimulator,\n",
    "    EvaluatorLog,\n",
    "    SimulationTape,\n",
    "    random_env_generator,\n",
    ")\n",
    "from relai.critico import Critico\n",
    "from relai.critico.evaluate import Evaluator\n",
    "from relai.logger import tracer_provider\n",
    "from relai.maestro import Maestro, params, register_param\n",
    "from relai.mocker import MockTool, Persona\n",
    "from relai.simulator import simulated\n",
    "\n",
    "# ---- Observability (optional but recommended) -------------------------------\n",
    "OpenAIAgentsInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "\n",
    "AGENT_NAME = \"Stock Chatbot\"\n",
    "MODEL = \"gpt-5-mini\"  # swap as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1 — Decorate inputs/tools that will be simulated\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "@simulated\n",
    "async def get_user_query() -> str:\n",
    "    \"\"\"Get user's query about stock prices.\"\"\"\n",
    "    # In a real agent, this function might get input from a chat interface.\n",
    "    # Since we are simulating this function, we return a fixed query.\n",
    "    return \"What is the current price of AAPL stock?\"\n",
    "\n",
    "\n",
    "@function_tool\n",
    "@simulated\n",
    "async def retriever(query: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    A retriever tool that returns relevant financial data for a given query about stock prices.\n",
    "\n",
    "    Args:\n",
    "        query (str): A question about stock prices.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of relevant financial data.\n",
    "    \"\"\"\n",
    "    # In a real implementation, this function would query a financial database or API.\n",
    "    # Since we are simulating this tool, we return an empty list.\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0932a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2 — Your agent core\n",
    "# (additional) To optimize in STEP 5.4, use `register_param` to define tunable\n",
    "# parameters and `params` to access them in your agent.\n",
    "# ============================================================================\n",
    "\n",
    "register_param(\n",
    "    \"prompt\",\n",
    "    type=\"prompt\",\n",
    "    init_value=\"You are a helpful assistant for stock price questions.\",\n",
    "    desc=\"system prompt for the agent\",\n",
    ")\n",
    "\n",
    "\n",
    "async def stock_price_chatbot(question: str) -> dict[str, str]:\n",
    "    agent = Agent(\n",
    "        name=AGENT_NAME,\n",
    "        instructions=params.prompt,  # access registered parameter\n",
    "        model=MODEL,\n",
    "        tools=[retriever],\n",
    "    )\n",
    "    result = await Runner.run(agent, question)\n",
    "    return {\"answer\": result.final_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3 — Wrap agent for simulation traces\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "async def agent_fn(tape: SimulationTape) -> AgentOutputs:\n",
    "    question = await get_user_query()\n",
    "    tape.agent_inputs[\"question\"] = question  # trace inputs for later auditing\n",
    "    return await stock_price_chatbot(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4 — Define evaluators (Critico)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class PriceFormatEvaluator(Evaluator):\n",
    "    \"\"\"An illustrative evaluator that checks for correct price formats in the agent's answer.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(name=\"PriceFormatEvaluator\", required_fields=[\"answer\"])\n",
    "\n",
    "    async def compute_evaluator_result(self, agent_log: AgentLog) -> EvaluatorLog:\n",
    "        bad_pattern = r\"\\$(?!\\d{1,3}(?:,\\d{3})+|\\d+\\.\\d{2}\\b)\\S+\"\n",
    "        bad_prices = re.findall(bad_pattern, agent_log.agent_outputs[\"answer\"])\n",
    "        score = 0.0 if bad_prices else 1.0\n",
    "        feedback = (\n",
    "            (\"Incorrect price formats found: \" + \", \".join(bad_prices)) if bad_prices else \"Price formats look good.\"\n",
    "        )\n",
    "        return EvaluatorLog(evaluator_id=self.uid, name=self.name, outputs={\"score\": score, \"feedback\": feedback})\n",
    "\n",
    "\n",
    "# (You can add built-in RELAI platform evaluators here as well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook example, since the agent code is contained in the notebook,\n",
    "# we create a source.py file containing the agent code for later optimization of agent structure\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "def get_notebook_code():\n",
    "    ip = get_ipython()\n",
    "    cells = ip.user_ns['In']  # This is a list of all executed input cells as strings\n",
    "    source = \"\"\n",
    "\n",
    "    # For example, print everything except the current cell\n",
    "    for idx, code in enumerate(cells):\n",
    "        if code and not code.strip().startswith(\"get_ipython()\") and not \"import os\" in code:\n",
    "            source += code + \"\\n\"\n",
    "\n",
    "    return source\n",
    "\n",
    "with open(\"source.py\", \"w\") as f:\n",
    "    f.write(get_notebook_code())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a497f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5 — Orchestrate: simulate → evaluate →  optimize\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    # 5.1 — Set up your simulation environment\n",
    "    # Bind Personas/MockTools to fully-qualified function names\n",
    "    env_generator = random_env_generator(\n",
    "        config_set={\n",
    "            \"__main__.get_user_query\": [Persona(user_persona=\"A polite and curious user.\")],\n",
    "            \"__main__.retriever\": [MockTool(model=MODEL)],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    async with AsyncRELAI() as client:\n",
    "        # 5.2 — SIMULATE\n",
    "        simulator = AsyncSimulator(agent_fn=agent_fn, env_generator=env_generator, client=client)\n",
    "        agent_logs = await simulator.run(num_runs=1)\n",
    "\n",
    "        # 5.3 — EVALUATE\n",
    "        critico = Critico(client=client)\n",
    "        critico.add_evaluators({PriceFormatEvaluator(): 1.0})\n",
    "        critico_logs = await critico.evaluate(agent_logs)\n",
    "\n",
    "        # Publish evaluation report to the RELAI platform\n",
    "        await critico.report(critico_logs)\n",
    "\n",
    "        # 5.4 — OPTIMIZE with Maestro\n",
    "        maestro = Maestro(client=client, agent_fn=agent_fn, log_to_platform=True, name=AGENT_NAME)\n",
    "        maestro.add_setup(simulator=simulator, critico=critico)\n",
    "        # one can use multiple simulator+critico setups with different weights by calling `add_setup` multiple times\n",
    "        # maestro.add_setup(simulator=simulator, critico=critico, weight = 1)\n",
    "        # maestro.add_setup(simulator=another_simulator, critico=another_critico, weight = 0.5)\n",
    "\n",
    "        # 5.4.1 — Optimize agent configurations (the parameters registered earlier in STEP 2)\n",
    "        # params.load(\"saved_config.json\")  # load previous params if available\n",
    "        await maestro.optimize_config(\n",
    "            total_rollouts=80,  # Total number of rollouts to use for optimization.\n",
    "            batch_size=4,  # Base batch size to use for individual optimization steps. Defaults to 4.\n",
    "            explore_radius=3,  # A positive integer controlling the aggressiveness of exploration during optimization.\n",
    "            explore_factor=0.5,  # A float between 0 to 1 controlling the exploration-exploitation trade-off.\n",
    "            verbose=True,  # If True, additional information will be printed during the optimization step.\n",
    "        )\n",
    "        params.save(\"saved_config.json\")  # save optimized params for future usage\n",
    "\n",
    "        # 5.4.2 — Optimize agent structure (changes that cannot be achieved by setting parameters alone)\n",
    "        await maestro.optimize_structure(\n",
    "            total_rollouts=5,  # Total number of rollouts to use for optimization.\n",
    "            code_paths=[\"source.py\"],  # A list of paths corresponding to code implementations of the agent.\n",
    "            verbose=True,  # If True, additional information will be printed during the optimization step.\n",
    "        )\n",
    "\n",
    "# asyncio.run(main()) # for python\n",
    "await main() # for notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
