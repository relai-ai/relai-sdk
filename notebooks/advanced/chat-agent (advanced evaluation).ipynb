{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Prereqs:\n",
    "#   export RELAI_API_KEY=\"relai-...\"        # your RELAI API key\n",
    "#   export OPENAI_API_KEY=\"sk-...\"          # if your agent/tool uses OpenAI\n",
    "#   pip install relai                       # relai\n",
    "#   pip install langchain                   # langchain\n",
    "#   pip install langchain-openai            # langchain-openai\n",
    "#\n",
    "# Here we demonstrate with a simple chat agent:\n",
    "# How to use different evaluators based on arbitrary criteria for agent optimization through the use of evaluator groups.\n",
    "\n",
    "!pip install relai\n",
    "!pip install langchain\n",
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e5683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"RELAI_API_KEY\"] = \"relai-...\"  # or set permanently in your system\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # or set permanently in your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a772034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from collections.abc import Callable\n",
    "from typing import Literal\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "from relai import AgentLog, AsyncRELAI, EvaluatorLog, simulated\n",
    "from relai.critico import Critico\n",
    "from relai.critico.evaluate import Evaluator\n",
    "from relai.data import SimulationTape\n",
    "from relai.maestro import Maestro, params, register_param\n",
    "from relai.mocker.persona import Persona\n",
    "from relai.simulator import AsyncSimulator, random_env_generator\n",
    "from relai.utils import log_model\n",
    "\n",
    "\n",
    "@simulated\n",
    "async def get_user_input(agent_response: str | None = None):\n",
    "    msg = input(\"User: \")\n",
    "    return msg\n",
    "\n",
    "\n",
    "register_param(\n",
    "    \"model\",\n",
    "    type=\"model\",\n",
    "    init_value=\"gpt-4.1-mini\",\n",
    "    desc=\"LLM model for the agent\",\n",
    "    allowed=[\"gpt-4o-mini\", \"gpt-4.1-mini\", \"gpt-5-mini\"],\n",
    ")\n",
    "register_param(\"prompt\", type=\"prompt\", init_value=\"You are a helpful assistant.\", desc=\"system prompt for the agent\")\n",
    "\n",
    "\n",
    "async def chat_agent(messages: list[dict]):\n",
    "    agent = create_agent(\n",
    "        model=params.model,\n",
    "        system_prompt=params.prompt,\n",
    "    )\n",
    "    response = agent.invoke({\"messages\": messages})  # type: ignore\n",
    "\n",
    "    response = response[\"messages\"][-1].content\n",
    "\n",
    "    # customized logging\n",
    "    log_model(\n",
    "        name=params.model,\n",
    "        input={\n",
    "            \"messages\": messages,\n",
    "            \"prompt\": params.prompt,\n",
    "        },\n",
    "        output=response,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "async def agent_fn(tape: SimulationTape):\n",
    "    total_response_time = 0\n",
    "\n",
    "    input = await get_user_input()\n",
    "    response = \"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": input}]\n",
    "    turns = 0\n",
    "    while \"[GOOD]\" not in input and \"[BAD]\" not in input and turns < 3:\n",
    "        turns += 1\n",
    "        # print(\"User:\", input)  # Debug print\n",
    "        tape.agent_inputs[\"user_text\"] = input  # trace inputs for later auditing\n",
    "        time_start = time.perf_counter()\n",
    "        response = await chat_agent(messages)\n",
    "        time_end = time.perf_counter()\n",
    "        total_response_time += time_end - time_start\n",
    "        input = await get_user_input(response)\n",
    "        # print(\"Agent:\", response)  # Debug print\n",
    "        messages.extend([{\"role\": \"assistant\", \"content\": response}, {\"role\": \"user\", \"content\": input}])\n",
    "\n",
    "    tape.add_record(\"conversation\", messages)  # record full trajectory in tape for evaluation\n",
    "    # ============================================================================\n",
    "    # STEP 1: One can set the evaluator group based on any criteria on the fly,\n",
    "    # and the evaluators added to Critico (see below) with the respective group will \n",
    "    # be used for evaluation automatically.\n",
    "    # ============================================================================\n",
    "    tape.set_evaluator_group(\"slow\" if total_response_time > 10 else \"fast\")\n",
    "\n",
    "    return {\"response\": response}\n",
    "\n",
    "\n",
    "class ConversationEvaluator(Evaluator):\n",
    "    \"\"\"\n",
    "    A custom evaluator for a conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transform: Callable | None = None, mode: Literal[\"rigid\", \"lenient\"] = \"rigid\"):\n",
    "        \"\"\"\n",
    "        Initialize the custom sentiment evaluator.\n",
    "\n",
    "        Args:\n",
    "            transform: Optional function to transform agent outputs\n",
    "            mode: Evaluation mode, either \"rigid\" or \"lenient\".\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=\"conversation-evaluator\",\n",
    "            # Specify required fields from the benchmark and agent response\n",
    "            required_fields=[\"conversation\"],\n",
    "            transform=transform,\n",
    "            # Store configuration as hyperparameters\n",
    "            mode=mode,\n",
    "        )\n",
    "\n",
    "    async def compute_evaluator_result(self, agent_log: AgentLog) -> EvaluatorLog:\n",
    "        \"\"\"\n",
    "        Evaluate the agent's behavior based on the user feedback at the end of a conversation.\n",
    "\n",
    "        Args:\n",
    "            agent_log (AgentLog): The response from the AI agent, containing the original sample\n",
    "                and agent outputs.\n",
    "\n",
    "        Returns:\n",
    "            EvaluatorLog: Evaluator log with score and feedback\n",
    "        \"\"\"\n",
    "        # Extract required fields from different sources\n",
    "        conversation = agent_log.simulation_tape.extras[\"conversation\"]\n",
    "        final_user_message = conversation[-1][\"content\"]\n",
    "\n",
    "        if \"[GOOD]\" in final_user_message:\n",
    "            score = 1.0\n",
    "            feedback = \"The agent was helpful.\"\n",
    "        elif \"[BAD]\" in final_user_message:\n",
    "            score = 0.0 if self.hyperparameters[\"mode\"] == \"rigid\" else 0.5\n",
    "            feedback = \"The agent could do better.\"\n",
    "        else:\n",
    "            score = 0.5\n",
    "            feedback = \"The conversation ended without clear feedback.\"\n",
    "\n",
    "        return EvaluatorLog(\n",
    "            evaluator_id=self.uid,\n",
    "            name=self.name,\n",
    "            outputs={\"score\": score, \"feedback\": feedback},\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f654728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook example, since the agent code is contained in the notebook,\n",
    "# we create a source.py file containing the agent code for later optimization of agent structure\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "def get_notebook_code():\n",
    "    ip = get_ipython()\n",
    "    cells = ip.user_ns['In']  # This is a list of all executed input cells as strings\n",
    "    source = \"\"\n",
    "\n",
    "    # For example, print everything except the current cell\n",
    "    for idx, code in enumerate(cells):\n",
    "        if code and not code.strip().startswith(\"get_ipython()\") and not \"import os\" in code:\n",
    "            source += code + \"\\n\"\n",
    "\n",
    "    return source\n",
    "\n",
    "with open(\"source.py\", \"w\") as f:\n",
    "    f.write(get_notebook_code())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869025d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def main():\n",
    "    env_generator = random_env_generator(\n",
    "        {\n",
    "            \"__main__.get_user_input\": [\n",
    "                Persona(\n",
    "                    user_persona=(\n",
    "                        \"You have a single random question to ask an agent. Do not ask follow up questions. \"\n",
    "                        \"When you think you get your answer and the conversation is over (or that it has been more than 3 turns), \"\n",
    "                        \"output [GOOD] if the agent was helpful, [BAD] if the agent could do better.\"\n",
    "                    ),\n",
    "                ),\n",
    "                Persona(\n",
    "                    user_persona=(\n",
    "                        \"You want to know what date today is by talking to an agent. \"\n",
    "                        \"When you think you get your answer and the conversation is over (or that it has been more than 3 turns),, \"\n",
    "                        \"output [GOOD] if the agent was helpful, [BAD] if the agent could do better.\"\n",
    "                    )\n",
    "                ),\n",
    "            ]\n",
    "        }\n",
    "        # Alternatively, set up a Persona Set through RELAI platform (platform.relai.ai) and use the code below:\n",
    "        # {\"__main__.get_user_input\": PersonaSet(persona_set_id=\"your_persona_set_id_here\")}\n",
    "    )\n",
    "\n",
    "    async with AsyncRELAI() as client:\n",
    "        simulator = AsyncSimulator(\n",
    "            client=client,\n",
    "            agent_fn=agent_fn,\n",
    "            env_generator=env_generator,\n",
    "            log_runs=True,\n",
    "        )\n",
    "\n",
    "        agent_logs = await simulator.run(num_runs=1)\n",
    "        print(agent_logs)\n",
    "\n",
    "        # ============================================================================\n",
    "        # STEP 2: One can add multiple evaluators with different weights to\n",
    "        # different evaluator groups.\n",
    "        # Evaluators added without specifying a group will be added to the default\n",
    "        # group, which is used for all runs with default evaluator group (i.e., no\n",
    "        # evaluator group set in the simulation tape via set_evaluator_group).\n",
    "        # ============================================================================\n",
    "        critico = Critico(client=client)\n",
    "        critico.add_evaluators(\n",
    "            evaluators={ConversationEvaluator(mode=\"lenient\"): 1, ConversationEvaluator(mode=\"rigid\"): 0.5},\n",
    "            evaluator_group=\"fast\",\n",
    "        )\n",
    "        critico.add_evaluators(evaluators={ConversationEvaluator(mode=\"rigid\"): 1}, evaluator_group=\"slow\")\n",
    "\n",
    "        # OPTIMIZE with Maestro\n",
    "        maestro = Maestro(client=client, agent_fn=agent_fn, log_to_platform=True, name=\"Evaluator Group Example\")\n",
    "        maestro.add_setup(simulator=simulator, critico=critico)\n",
    "\n",
    "        # Optimize agent configurations (the parameters registered previously)\n",
    "        # params.load(\"saved_config.json\")  # load previous params if available\n",
    "        await maestro.optimize_config(\n",
    "            total_rollouts=10,  # Total number of rollouts to use for optimization.\n",
    "            batch_size=2,  # Base batch size to use for individual optimization steps. Defaults to 4.\n",
    "            explore_radius=1,  # A positive integer controlling the aggressiveness of exploration during optimization.\n",
    "            explore_factor=0.5,  # A float between 0 to 1 controlling the exploration-exploitation trade-off.\n",
    "            verbose=True,  # If True, additional information will be printed during the optimization step.\n",
    "        )\n",
    "        params.save(\"saved_config.json\")  # save optimized params for future usage\n",
    "\n",
    "        # Optimize agent structure (changes that cannot be achieved by setting parameters alone)\n",
    "        await maestro.optimize_structure(\n",
    "            total_rollouts=10,  # Total number of rollouts to use for optimization.\n",
    "            code_paths=[\"source.py\"],  # A list of paths corresponding to code implementations of the agent.\n",
    "            verbose=True,  # If True, additional information will be printed during the optimization step.\n",
    "        )\n",
    "\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067a155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
